A1
==
学校编码:10384
分类号	密级

学 号:X2011230995
	UDC—

ji灼氟`i

工程硕士学位论文

基于网络爬虫的信息采集分类系统

设计与实现

Designing and Implementation of Information Collection

and Classification System Based on Web Crawler

指导教师:
刘昆宏
副教授

专业名称:
软件
X
程

论文提交日期:
2013 年
10
月

论文答辩日期:
2013 年
11
月

学位授予曰期:
2013 年
月

>;
指导教师:士 \、a

答辩委员会主席:
	!`'

2013年 月
厦门大学学位论文原创性声明

本人呈交的学位论文是本人在导师指导下,独立完成的研究成

果。本人在论文写作中参考其他个人或集体已经发表的研究成果,

均在文中以适当方式明确标明,并符合法律规范和《厦门大学研究

生学术活动规范(试行)》。

另外,该学位论文为(

(组)的研究成果,获得(

验室的资助,在(

)课题

)课题(组)经费或实

)实验室完成。(请在以上括号

内填写课题或课题组负责人或实验室名称,未有此项声明内容的,

可以不作特别声明。)

声明人(签名):

糾3年/丨月如日
厦门大学学位论文著作权使用声明

本人同意厦门大学根据《中华人民共和国学位条例暂行实施办

法》等规定保留和使用此学位论文,并向主管部门或其指定机构送

交学位论文(包括纸质版和电子版),允许学位论文进入厦门大学图

书馆及其数据库被查阅、借阅。本人同意厦门大学将学位论文加入

全国博士、硕士学位论文共建单位数据库进行检索,将学位论文的

标题和摘要汇编出版,采用影印、缩印或者其它方式合理复制学位

论文。

本学位论文属于:

( )1.经厦门大学保密委员会审查核定的保密学位论文,

于 年月 日解密,解密后适用上述授权。

(心)2.不保密,适用上述授权。

(请在以上相应括号内打“V”或填上相应内容。保密学位论

文应是已经厦门大学保密委员会审定过的学位论文,未经厦门大学

保密委员会审定的学位论文均为公开学位论文。此声明栏不填写的,

默认为公开学位论文,均适用上述授权。)

声明人(签名):

>0/4年//月%日
摘要

在互联网走进世界每一个角落的今天,互联网信息在不断地膨胀,每日互

联网将产生大量的数据,其中涵盖了每天发生发展的各种各样的事件,可谓覆

盖人们生产生活的方方面面,这其中包含了大量富有价值的数据,同时又有绝

大部分我们不关心的数据,如何从如此海量的信息中抽取有价值的数据,是我

们急需思考的问题。

系统使用蜘蛛爬虫技术,结合实际需求 发互联网采集系统,使用定向釆

集思想,快速定位釆集符合业务需求的互联网数据,然后将釆集结果数据通过

文本聚类,归类出符合特性条件的数据集合,以方便后续其他业务的数据支持。

本系统釆用java语言面向对象的思想,lucene搜索引擎技术做底层数据检索支

持,开源的中文分词器IK,应用方面实现SSH经典Web开发框架,展现一个

简单的互联网信息采集分类系统。

系统能够为有互联网数据分析需求的个人、企业或者政府提供需求数据的

先期过滤聚类,为各种复杂业务的数据分析提供一期标准化数据,在当今这个

数据时代里,能发挥很好的作用。

关键词:信息采集;网络爬虫;Lucene

I
Abstract

With the development of internet, the online data grows at high speed every day,

including all kinds of news, pictures, videos,covering all aspects of human life and

becoming more valuable for users. Hence, how to extract useful information from

massive date for fully using is becoming an extremely urgent question.

This dissertation discusses the use of spider crawler technology and develops an

information collection system focusing on actual demands. The system is based on

directional collection methods with quick positioning scheme through Web. It aims

to meet business requirements, and then cluster the results of text collection for

further consideration of business.

In order to facilitate the data support of the follow-up of other business, the

system applies JAVA language and Lucene Search Technology. Chinese division

machine IK is the underlying data retrieval support. All is aimed to construct a

simple net information collection classification system.

The system provides prefiltering and date clustering for individuals, companies

and governments that need internet date analysis. It also provides a standardized date

to cope with a variety of complex bussiness date analysis. In this date age, the

systerm can play a very important role.

Key words: Information Collection; Web Crawler; Lucene

III
第一章绪论

1.1论文研究背景与意义
	




1.2本课题研究内容与目标	




1.3论文内容的组织
	




第二章相关技术简介
	




2.1	Java语言的特点
	




2.2	J2EE
	




2.3	Lucene 简介
	




2.4关于自然语言分词算法	




2.5 SSH 框架
	




2.6信息分类算法
	




2.7爬虫技术
	




2.7.1爬虫分类
	




2.7.2网络爬虫的两种工作方式

2.7.3 Web页面信息抽取技术....

2.7.4本系统爬虫技术
	




2.8存储环境介绍
	




2.8.1关系数据库Oracle	




2.8.2分布式存储Hbase	




2.8.3 key-value 存储系统 Redis.

2.9本章小结
	




第三章需求分析
	




3.1可行性分析
		

3.2功能需求分析
	




3.2.1爬虫业务需求分析	




3.2.2数据处理需求分析	




3.2.3数据展示需求分析	




3.3性能需求分析
	




3.3.1数据釆集效率
	




3.3.2数据分类效果
	




3.4其他需求
	




3.5本章小结
	




第四章系统设计
	




4.1系统框架设计
	




4.2爬虫设计
	




4.3数据分类逻辑设计
	




4.4	WEB展示设计
	




4.5数据结构设计
	



4.5.1数据关系模型
	31

4.5.2实体一关系模型
	32

4.5.3数字结构
	33

4.6本章小结
	36

第五章系统实现
	37

5.1开发环境的搭建
	37

5.2系统功能实现
	37

5.2.1数据釆集系统
	37

5.2.2数据分类
	45

5.2.3 WEB 展示
	50

5.3系统测试
	53

5.4系统出错处理
	56

5.4.1出错信息
	56

5.4.2补救措施
	57

5.5本章小结
	57

第六章总结与展望
	58

6.1总结	58

6.2展望	58

参考文献	59

^	60

VI
Contents

Chapter 1 Introduction
	1

1.1	Background and Significance
	1

1.2	Objectives and Contents
	1

1.3	The structure of this dissertation
		
	2

Chapter 2 System related technologies outline
	3

2.1	The Features of the Java Language
	3

2.2	J2EE
		
	4

2.3	Introduction of Lucene
	5

2.4	A Natural Language Parsing Algorithm
	6

2.5	The SSH Framework
		
	7

2.6	Information Classification Algorithm
		
	8

2.7	Crawler Technology
		
	9

2.7.1	Crawler Classification
			9

2.7.2	Two Kinds of Work Mode of Web Crawler
	.-.	10

2.7.3	The Technology of Web Information Extraction
	12

2.7.4	Crawler Technology in this System
	13

2.8	Storage Environment
	13

2.8.1	RDB Oracle
	13

2.8.2	Distributed Storage Hbase
	15

2.8.3	Key-value Storage Redis
	17

2.9	Summary
	19

Chapter 3 Requirement Analysis
	20

3.1	Feasibility Analysise		
		
			..........20

3.2	Functional Requirement Analysis
	21

3.2.1	Crawler Business Requirement Analysis
	21

3.2.2	Data Analyse Requirement Analysis
		
	..22

3.2.3	Data Show Requirement Analysis
	23

3.3	Performance Requirement Analysis
	23

3.3.1	The Data Collection Efficiency
	23

3.3.2	Data for Classification
		
		
	24

3.4	Other Requirements
	24

3.5	Summary		
	25

Chapter 4 system design
	26

4.1	System Framework Design
		
	26

4.2	Crawler Design
	27

4.3	Logic Design of Data Aanalysis
	30

4.4	WEB Display Design
	31

VII
4.5	Data Design
		
	31

4.5.1	Relation Data Model
	31

4.5.2	Entity Relationship Model
			32

4.5.3	Digital Architecture
	33

4.6	Summary
			36

Chapter 5 System Implementation
	37

5.1 Set Up Development Environment
	37

5.1	The Realization of System Function
		
	37

5.1.1	Data Acquisition System
	37

5.1.2	Data Classification
	.....45

5.1.3	WEB Show
	50

5.3 System Test
		
			53

5.2	Error Processing System	
				
	56

5.2.1	Error Message
	56

5.2.2	Remedial Measure
		
	57

5.3	Summary.		
		
		
	57

Chapter 6 Conclusions and Prospect
		 58

6.1	Conclusions
		
	58

6.2	Prospect
			58

References	59

Acknowledgements 
		
	60

VIII
	第一章绪论
	




第一章绪论

1.1论文研究背景与意义

当前,互联网作为人们生活中不可或缺的一部分正在发挥重要作用,人们

每天的生活信息都存储在这个庞大的互联网里,这里充斥着海量的信息。互联

网信息对于我们生活的重要性是突出的:从国家政府方面讲,政府能够快速从这

些信息里发现政策制度存在的问题,以得到及时的修正;在舆论爆发到不可收

拾前,得到正确的信息,做出适当决策,以保证社会稳定。从企业发展作用方

面讲,透过错综复杂的互联网信息,从中得到真正的商机,是企业发展的利器;

大企业在面对消费的评价,能够更加直接透明地了解消费者的心态,以及消费

的倾向,因此才能做出正确的企业发展策略,保证企业立于长期稳定发展中。

从个人方面讲,在互联网的不断膨胀过程中,论坛、微博等的出现,使得读报

看新闻已不再能看透社会的总体观点走向。人们需要更多的信息去理解这个社

会,让自己更多的了解当前的社会的发展状况。互联网的信息丰富而多样,她

服务于整个社会的各行各业,所有我们需要的知识数据,绝大部分可以从互联

网上获知。但是当我们需要对属于我们有用的信息做分析时,庞大的数据量,

让我们对其望而却步,这就需要一个网络信息采集分类系统去完成实现,这就

是本课题研究的意义。

1.2本课题研究内容与目标

当前,我们已经进入了大数据时代。从技术上说,大数据包括如Hadoop、

高扩展度数据库以及高性能搜索引擎这样的新技术。全球对大数据技术和服务

的投资正在增长。目前,大数据在美国最为发达,其他地区在这一领域稍显落

后。即使如此,大数据的挖掘技术仍然相对薄弱,目前国内外的大数据分析仅

一些大的企业才能达到预定的分析挖掘能力。究其原因,一方面是由于大数据

需要很强的硬件支撑,另外一方面也是因为此种技术的支撑还不够成熟,也未

能达到很好的普及应用。

本课题是一个相对轻量级的大数据分析模型,其目的能做一些基础的数据

获取分析功能,达到一些简单的预定分析能力。本课题采用面向对象的方法,

运用J2EE、数据库、lucene、SSH架构等现有的技术,设计开发一个网络信息

1

7
	基于网络爬虫的信息釆集分类系统设计与实现
	




采集分类系统,即通过网络爬虫技术釆集互联网信息,然后根据一定的规则,

对信息进行处理分类展示,这就是本课题研究的内容和目标

1.3论文内容的组织

全文共分六章,具体结构安排如下:

第一章绪论,简要介绍项目背景、研究意义,课题的研究内容、论文内容

的组织。

第二章相关技术综述,对实现系统需要用到的几个关键技术进行论述,包

括分词算法、爬虫技术、数据存储等。

第三章系统需求分析,主要论述功能需求、性能需求、其他需求。

第四章系统设计,基于需求分析,设计各个模块的详细业务处理流程,包

括爬虫业务、分析业务、Web展示。

第五章系统实现,讲述了整个系统的实现,涵盖系统框架设计,功能详细

代码实现以及异常处理等。

第六章总结与展望,本章对研究的工作进行总结,并对将来进一步的工作

进行展望。

2
第二章相关技术简介

□ —
 '□`
 -
 □ -..
 □. —
 、

第二章相关技术简介

2.1 Java语5的特点

1、平台无关性

Java运行环境提供了平台无关性是其一个最大的优点,其釆用虚拟机原

理,所有的Java程序都运行于虚拟机之上,使之不必关心软硬件环境的影响,

实现不同平台之间的Java接口,达到平台间最大限度的兼容,使用Java编写

的程序能在不同的平台运行。Java所定义的数据类型和机器环境无关,在Java

的规范中没有“依赖具体实现”的概念,为了保障Java跨平台特性,Java中基

本数据类型的算法和大小都做了严格的规定。

2、安全性

Java的安全性体现在两个方面。一方面,相比于C++,由程序员直接操作

内存的方式,很容易产出一些不可预料的程序bug,如忘记释放内存或试图访

问已经释放的内存,在Java语言里,关于C++中象指针和释放内存等功能被废

除,从而降低了非法内存操作的可能,使程序员无需关心内存细节而更专注于

程序业务逻辑的实现。另一方面,用Java来构建浏览器的时候,Java语言功能

有效地和浏览器本身的功能相结合,使它更安全。Java语言在执行前,要通过

很多次的检查测试。它经过严格代码校验,检测指针操作,检查代码段的格式。

3、面向对象

Java很好地参考了 C++面向对象的概念,釆用类将封装数据和操作,实现

了程序的简洁性和便于维护性,使程序代码可以只需一次编译就可反复利用。

90年代的软件开发方法是以面向对象为主流,期间面向对象得到了广泛的应用

和实践,从而整理它是一种很好的软件开发思想,目前面向对象思想已不仅仅

只局限于软件 发,而更多的应用于其他领域。Java语言采用面向对象的思想

促使其迅速发展。

4、分布式

在Java里有一个可以支持基于TCP/IP协议的子库。所以,Java应用程序

可直接使用URL访问网络上的对象,与其访问本地文件系统的方式几乎完全相

同。Java的语法的这一特性使我们很轻松地为分布环境特别是Internet提供的

3
	基于网络爬虫的信息采集分类系统设计与实现
	




动态相关内容。

5、健壮性

Java对程序代码能够进行很好的自我检查,及时发现程序在编译和运行时

的错误。许多开发早期出现的错误可以通过类型检查帮助检查出来。同时Java

自己操纵和管理内存,减少了程序员误操作内存导致出错的概率。Java的真数

组,可避免了覆盖数据。这些特征能够将开发程序的周期最大限度地缩短。Java

还提供空指针检测、数组边界检测、异常出口字节等的代码校验。

6、多线程

Java提供多线程处理,处理逻辑相对简单,同时跨平台上的多线程代码调

用方式相同。该特点可以方便程序员开发基于多线程的业务,使程序很好的利

用系统资源,达到更好的实时交互相应功能。

2.2 J2EE

J2EE有别于传统应用开发的技术架构,其中包含许多组件,主要可简化且

规范应用系统的开发与部署,进而提高可移植性、安全与再用价值。

J2EE以一组技术规范与指南作为核心思想,其中所包含的各类相关组件、

技术层次及服务架构,都有共同的标准及规格,所以,基于J2EE架构的应用在

不同平台间,可以有良好的兼容性,统一的规范可以解决一些应用服务或者产

品之间无法兼容的问题,同时也解决了企业内外互通难题。

J2EE体系结构在中间层做了集成框架,可以在费用有限的情况下满足高可

用性、高可靠性以及可扩展性的应用的需求。通过提供统一的 发平台,J2EE

使得多层应用开发的费用和复杂性得到最大限度的降低,同时能够很好的对现

有应用程序进行集成,完全支持Enterprise JavaBeans,方面快捷的向导支持打

包和部署应用。

四层系统:

1、UI层:借助Struts实现

2、业务层:借助SpringFramework对业务组件的组装进行关联

3、数据持久层:借助Hibernate[8]实现

4、域对象层:将所有域对象划分在一个层面

采用这样的四层架构的优势:

4
	第二章相关技术简介
	




1、通过J2EE框架实现多次应用的开发相对于自己编写代码实现,能够极

大缩短开发周期,且架构较为成熟稳定,拥有很广泛的用户群,饱经实践考验,

具有良好的质量和性能的保障。

2、层与层之间松散稱合,增加代码重用率。

3、各层分工明确,这样也利于团队的明确分工。

2.3 Lucene 简介

Lucene作为一个全文检索引擎,其具有如下突出的优点:

1、索引文件格式独立于应用平台。Lucene定义了一套索引文件格式,该文

件格式以8位字节为基础,使得不同平台的应用或者兼容得以共享建立的索引

文件。

2、传统的全文检索引擎是以倒排索引为核心,在此基础上,Luncene又实

现了分块索引,能够对新增的文件建立小文件索引,提升索引效率。然后将小

文件索引与原有索引进行合并,优化检索引擎。
	r

3、	Lucene是建立在面向对象的基础上的,由于面向对象思想已经非常成熟,

使得对Lucene的学习难度有效降低,同时,程序员也可方便扩充新功能。

4、设计了独立于语言和文件格式的文本分析接口,索引器通过接受Token[3]

流完成索引文件的创立,用户扩展新的语言和文件格式,只需要实现文本分析	-

的接口。	`

5、Lucene已然实现了一套强大的搜索引擎,用户可直接利用已有的搜索功

能而无需自己编写,从而获得强大的查询能力。Lucene的查询实现中默认完成

了分组查询、模糊查询(Fuzzy Search).、布尔操作等等。

面对其他的商业全文检索引擎,Lucene也具以下几点优势。

首先,它的开放源代码,并且代码发行方式遵守Apache Software License[4],

因此程序员除了可以对Lucene所提供的强大检索功能充分利用以外,还可以深

入学习到关于全文检索引擎制作技术,也能在面相对象编程中得到很好的实践,

进而根据应用的实际情况编写扩展开发Lucene的扩展功能,开发出更加适合当

前应用业务的全文检索引擎。在灵活性这一点上,是其他的商业搜索引擎远远

不及的。

其次,Lucene充分发挥了开放源代码中的架构优良这一优势,其面向对象

5
	基于网络爬虫的信息采集分类系统设计与实现
	




架构在设计时就充分考虑合理性和易扩展性,良好的架构使程序员可以方便扩

展各种基于Lucene检索的功能,可以扩充中文的处理能力,从简单的文本扩展

到PDF、HTML等等其他文本格式的处理,由于有了 Lucene基础的支持,编写

这些扩展的功能变的简单,开发效率高,同时由于Lucene对系统设备做了程序

上的抽象处理,扩展的功能也具有良好的平台兼容性。

最后,Lucene加入到apache软件基金会后,在apache软件基金会的网络

平台上,程序员能够很好地与其他的开发者进行经验交流、共享资源,甚至可

以从其他开发者中获得符合需求的完备的扩充功能,这也促使Lucene向前推进

和发展。虽然Lucene使用Java语言写成,但在 源环境中,它也在向其他语

言中发展,开放源代码社区的程序员能够将其思想功能实现于其他各种传统语

言上(例如.net framework),以Lucene索引文件格式的规范为基础,使得Lucene

能在各大平台上具有很好的兼容性,系统管理员则可以根据平台的需要选择适

合的语言来构建全文搜索引擎。

2.4关于自然语言分词算法

对中文自然语言句子进行划分,从而得出有独立意义的词,这样的过程被

称为分词。众所周知,英文是以词为单位的,词和词之间是靠空格隔开,而中

文是以字为单位。由于中文语言的特殊性,词与词之间没有明确的界限,因此,

中文分词技术对中文信息处理往往要基于分类、机器翻译、搜索引擎以及信息

检索等相关技术。中文分词技术是自然语言处理技术的一个重要环节,是对中

文语句理解过程中最初要处理的一个环节,通过分词手段将组成语句的核心词

语提炼出来,然后提供给语句分析模块使用,在分词的过程中,如何提供最有

效的句子关键词语给分析模块至关重要,计算机对这一过程的处理就称为分词

算法。目前的分词算法主要可分为三大类。

1、基于字符串匹配的分词方法

通过字符串匹配的分词方法也同时叫机械匹配法,在分词技术的算法中,

该方法是较简单且比较实用的一种分词算法。它是按照一定的规则将待分析的

中文文章或者句子与一个"充分大的"机器词典中的词条进行区配,若词典中存

在某个字符串,则表示匹配出一个词语。一般包含的机械分词方法如下:最少

切分(使每一句中切出的词数最小);正向最大匹配法(由左到右的方向);逆

6
	第二章相关技术简介
	




向最大匹配法(由右到左的方向)。将上述各种方法相互组合设计出合理的分词

算法能达到更好的分词效果。

2、基于统计的分词方法

在很多情况下,由于中文组合随性还是相对比较大的,在限定的词库中对

不同文章进行匹配分词可能产生一些歧义,因此基于固有词库的匹配算法无法

解决分词歧义问题,那么就要基于不同的文章做自我的处理。这里可以釆用基

于统计的分词方法方进行处理。这种方法只需对文章中的字组出现频率进行统

计,不需要分词词典,从而无需整理和加载庞大的分词次库。从形式上看,词

是一个个字的组合,因此在上下文中,我们只需统计相邻的字同时出现的频率,

频率越高就越有可能构成一个词。在中文自然语言中,相邻的字共同出现的频

率越高,其成为一个词语的可能性就越大,可信度也较高。统计文章可中相邻

共现的各个字的组合的出现频率,计算它们的同时出现的概率。该指标能很好

的体现汉字之间关联的的紧密程序,从而判断分词。

3、基于理解的分词方法

大部分的分析系统,都是以消除所有歧义分词现象为目标。基于理解方式

的分词算法在后续自然语言分析过程中来能够较好处理歧义切分的问题。基本`


理解分词的方法的基本思想就是通过常规自然语言的语法信息,对句子进行分

析判断,语法信息其实是人对句子理解的一个参考方式,让机器学习釆用这样

的方式理解部分句子的语义,这样能够很好的解决机器死板的分析方I。理解

分词系统包括三个部分:句法语义子系统、分词子系统、总控部分。通过总控

部分进行协调,分词子系统中文自然语言的句法和语义信息来对分词进行歧义

判断,整个系统模拟了人通过语法对句子的理解过程。

2.5 SSH框架

SSH为struts+spring+hibemate[6]的一个集成框架,是目前较流行的一种

Web应用程序开源框架。

从职责上分,可以将SSH框架集成系统分为如下四层:表示层、业务逻辑

层、数据持久层和域模块层,以帮助开发人员在短期内搭建结构清晰、可复用

性好、维护方便的Web应用程序。其中以Stmts作为整个系统的基础架构,负

责MVC的分离,在框架的模型部分,利用Hibernate框架提供对持久层的支持,

7
	基于网络爬虫的信息采集分类系统设计与实现
	




业务层釆用Spring[5]支持。具体做法是:根据需求采用面向对象的分析方法提

出一些模型,并将这些模型实现为Java对象,然后编写基本的DAO接口,同

时给出Hibernate的DAO实现,结合Hibernate架构实现的DAO类来完成Java

类与数据库之间的转换和访问,而业务逻辑采用Spring完成。

系统的基本业务流程是:首先在表示层中釆用JSP[7]页面实现界面交互,负

责传递请求(Request[9])以及接收响应数据(Response[9]),而Struts则根据配置文

件(struts-config.xml)将 ActionServlet 接收到的 Request 委派给相应的 Action 处

理。在业务层中,服务组件的管理者Spring loC容器提供业务模型(Model)组件

给Action,结合该组件的协作对象即数据处理(DAO)组件完成业务逻辑,同时

提供缓冲池、事务处理等容器组件以提升系统性能,保证数据的完整性。在持

久层中,贝帷采用Hibernate的对象化映射和数据库进行交互,以此处理DAO

组件请求的数据和结果返回的业务。

釆用上述 发模型,实现了视图、模型以及控制器的分离,同时将展示层、

业务逻辑层、数据持久层都做了清晰的分离。这样可方便于工程的分离开发,

前端开发员无论如何变化,都不影响后台逻辑的开发,以及底层的数据结构;

同时数据库等的改动对前端也不会照成很大的影响。这样的开发模型大大提高

了系统的灵活性、复用性。同时降低各个业务层次之间稱合度,有利于团队成

员同时投入工作,大大提高了系统的开发效率。

2.6信息分类算法

该文本分类使用流式分类,即多层过滤,过滤流程如下:

1、杂质过滤

利用过滤词语料,例:股票、股票编码等;计算命中的过滤语料词个数,

进行综合加权,最后把得分最高的文档过滤掉,这样的好处是使网络爬取的文

档尽可能是用户所关注的;

2、索引创建

经过流程1后,对剩下的文档进行索引创建,索引利用的技术为lucene及

中文分词器IK;将文档内容进行中文切词,对切词后的每个词语在文档中出现

的位序进行倒排索引的创建,使页面在搜索框搜索的词语快速定位到相关的文

早;

8
	第二章相关技术简介
	




3、文本聚类

在数据库表中有设计两个字段,分别是包含(一般为行业专属词语,例:

煤矿,材料等)、不包含(一般为跟本行业无关的词语,例:木材,料理等),

在每个子孙节点中有设置网站名称,即:子孙节点必须包含设置好的网站名称、

必须包含的命中词并且不在不包含的词语内;父级节点包含所有子孙节点的数

据。

2,7爬虫技术

2.7.1爬虫分类

在网页釆集研究过程中,为解决其中的关键问题,研究者们通过不断地研

究与实践,Web爬行器不再仅仅基于整个Web的爬取,同时可以基于不同的需

求采用多种釆集技术开发特定的爬行器。总结起来,可以分为以下几种类型:

第一类是基于整个Web的爬行器。在一些大型门口网站的搜索引擎中,需
	'

要采集整个Web的数据信息,进而做索引提供整个Web的全文检索,#就需要

爬行器能够自动发现Web中的URL进而釆集其中信息,不断扩展信息量。这

类爬行器主要是从一些种子URL通过爬虫器自动发现新的URL从而扩充到整

个Web。该爬虫器的目标是采集整个Web,因此对硬件的要求比较高,需要足

够的内存以及存储数据的硬盘空间,而对采集页面的顺序要求相对较低。

第二类是增量式的爬行器。传统的爬行器根据设定的任务规则采集符合条

件的信息后就停止了,当需要对业务数据进行更新时,往往采用全部重新釆集

一遍的方式来替换旧的信息,这样的爬行器称为周期性Web釆集器。如果釆用

增量式爬行器对旧数据更新,其方式是对Web中增加的或者有变化的页面进行

釆集,而对无变化的页面釆集任务,予以丢弃,从而减少爬行器的任务量。同

周期性爬行器相比,增量式爬行器能极大地减小数据采集任务量,从而极大地

缩短了采集的时间,同时减少硬件资源开销。但是与此同时,由于增量式需要

更多的逻辑判定,因而也增加了爬行器开发算法的复杂性和技术难度。

第三类是基于主题的爬行器,该爬行器需要用户预先定义要相关的主题信

息,爬行器通过该主题信息有选择地搜索与该主题相关的页面。与基于整个Web

的爬行器相较,与主题无关的页面并不会纳入采集范围,由于该爬行器的运行

思想能够极大的减少资源开销,同时保证获取到用户关心的目标数据,所以这

9
	基于网络爬虫的信息采集分类系统设计与实现
	




一类的爬行器是目前研究的重点方向。但它同样存在很明显的问题,例如对主

题的内容如何定义,以及采集过程中主题与页面内容关联性要如何判断,如何

保证数据信息的精准性和完整性,都是有待解决的问题。

第四种是移动的爬行器。这种爬行器为减少自身的资源开销,直接废弃传

统的向Web服务器发请求的方式,而是主动将自己上载到服务器端,进行本地

釆集,最后将采集结果打包压缩再传输回本地。这类爬行器最大特点是将自己

分散出去采集,但是这也需要Web服务器环境允许的情况下才能做到。

第五种是基于元搜索的爬行器。目前互联网上已经存在了很多大型的门户

搜索网站,在一些特定的业务需求中,往往可以直接利用这些门户网站的元搜

索功能,用户需要的目标信息可以通过元搜索,釆集其中的信息,并结合各大

搜索平台提供的信息进行整合也能达到数据的最优完整度。但是这类爬行器由

于采集对象是大型门户的数据信息,其信息也是釆集而来,二次采集不可避免

的会与原始数据存在少部分的差距。

2.7.2网络爬虫的两种工作方式

1、集中式网络爬虫

集中式爬虫的工作原理是:首先爬取初始配置的URL种子集合,通过爬

虫主体程序釆集这些URL所对应的页面html,将html发送给页面内容分析模

块对其中的信息进行提取,将有效信息进行保存,同时获取页面中新的连接指

向,保存到任务URL集合中,以备后续继续爬取。集中式网络爬虫的釆集效率

相对较低,除简单业务外,已无法满足大量数据采集的业务需求。

2、分布式网络爬虫

随着Web信息的急剧增长,网络爬虫信息釆集的速度渐渐无法满足实际业

务的应用需要。即使是一些大型的信息采集系统,它对整个Web的覆盖率也只

能达到30-40%,对已经采集的页面更新一遍需要花数周到一个月的时间。当然

我们可以通过升级釆集系统的硬件信息达到一定的性能提升,使用更好的硬件

环境,搭建更好的计算机系统,保证日益庞大的业务需求,然而这样的方法只

能解决部分问题,同时其性价比很低,我们需要对爬行器进行釆集策略的提升,

用分布式方式来进行网页信息采集。分布式网络爬虫,能够倍数提升网络爬虫

的釆集效率,因为它是由一个个集中式网络爬虫节点构成,只要处理好各个节

10
	第二章相关技术简介
	




点间的信息通信,就能够构建出合理性能良好的爬虫系统。

3、分布式网络爬虫研究现状

目前网络爬虫比较成熟的应用,即在大型搜索引擎中,用户一般要通过搜

索引擎获取相关信息,这就需要搜索引擎的搜索库中包含尽可能多的网页,一

个搜索引擎的质量的好坏往往取决于搜索库中的网页数量。分布式技术以其高

效性的优点,被许多大型爬行器所釆用,它能够在短时间内搜集大量的网页信

息,是目前研制高效搜索引擎的关键技术,分布式爬行器的思想已经在各大门

户搜索站中得到很好的应用,但是由于涉及到商业机密,较好的分布式算法文

章还相对比较少,分布式釆集的理论思想还不够完善。目前,较有名的分布式

爬虫有 Google Crawler、UbiCrawler In> temet Archive Crawler> Mercator 等,

国内的有北大天网的Web Gather爬虫系统。

Google的分布式爬虫系统由四台机器组成,以中央主机作为任务分发节

点,与其他三台机器进行通信,分发其中的采集任务,其他机器采用,I/O异步

的策略并发从300个网站上获取网页信息,网页采集进程Crawler将下载下来

的网页源码压缩存储在磁盘上,提供Indexer进程读取,Index从这些网页源码

中提取新的连接信息,存放在另一磁盘文件中,然后URLResolver读取其中的

链接,将无法直接访问的相对链接转换为绝对链接地址,最后存入一个文件供

中央主机读取。Google的分布式爬虫的不足之处在于,一切程序活%受中央主

机的限制,中央主机一旦宕机,整个系统将停止工作,而且中央主机的任务分

发速度是整个系统的瓶颈所在。

Mercator是AltaVista搜索引擎的网络爬虫,它采用JAVA开发写成。通

常Java语言不如C++等语言的执行效率高,然而Java语言开发的Mercator的

具有良好可扩展性,可以通过增减或替换功能模块使不同的业务功能得到实现。

同时为了提升系统的性能,Mercator做了非常多的工作,它重写了 Java的核心

类库,采用缓存策略,高速的硬盘系统,使得Java也能开发出绝对高效的性能

的程序。另一方面,Mercator釆用的数据结构只占用有限的内存,大部分的数

据结构都在硬盘中存放。关于URL只存放其checksum值,最大限度节省了

内存和磁盘空间的消耗。Mercator对最近访问的URL做了缓存,该缓存的准

确率达到85%。

11
	基于网络爬虫的信息采集分类系统设计与实现
	




2.7.3 Web页面信息抽取技术

信息提取(Information Extraction,简称IE)的目标对子自然语言文档进行分

析,抽取其中的有效数据,方便进行结构化存储。IE系统通过一系列的信息抽

取规则以及抽取模式来确定自然语言文档中的有效信息。该技术有利于从大量

复杂数据中抽取出有效的关键信息。Web中的信息分散杂乱,并且涉及了生产

生活的各个方面,类似的信息通常出现在不同的网站上,同时表现形式也不尽

相同。采用那个信息抽取技术有效提取爬行器采集的信息,用于数据的结构化

存储。

Web 信息抽取(Web Information Extraction,简称为 WeblE)是将 Web 作为信

息源的一类信息抽取,Web文件是一种半结构化的数据类型,是Web信息挖掘

的数据来源。Web信息抽取基于传统的信息抽取技术,同时发展符合Web信息

提取业务需求的相关技术,将半结构化的Web文档信息处理的各加结构化,方

便信息挖掘等其他业务程序的数据调用。

目前的Web信息提取技术主要有以下几类:

(1)基于视觉特征的信息抽取

Web作为直接与用户进行交互的前台界面,其具有很好的视觉特性。视觉

特效即Web中的标签结构,颜色处理,区域划分等都对页面信息进行了有效的

归类,信息抽取技术即可以直接利用这样的视觉表示抽取其中的信息。由于Web

页面源码具有这样的视觉特性,源码中的颜色,字体大小,区域划分标签<div>,

<table>, <p>等成为了信息抽取技术定位数据的有效标识。

(2)基于wrapper的信息抽取

将特定数据源的抽取规则及其抽取需要的程序代码,封装为一个个包装器

(wrapper),将页面信息传递给包装器,返回符合业务要求的数据,即可完成信

息的抽取。由于一种类别的数据需要一个包装器的支持,所以该技术只符合部

分业务需求,否则将要应对大量的包装器的开发工作。

(3)基于HTML结构的信息抽取

Web的信息文档结构都为HTML结构,HTML具有良好的树形结构,因此,

可以通过将HTML转化为严格的树形文档结构,再基于树形结构实施信息的定

位提取。具体操作过程:通过HTML解析器将网页源码结构化处理生成树形结

12
	第二章相关技术简介
	




构文档对象,再根据实际的业务需求自动化或者半自动化生成信息提取规则,

最后利用规则完成目标信息的提取。

2.7.4本系统爬虫技术

网络爬虫通过某一个种子地址,通常为网站的首页地址,利用该地址釆集

网页,提取其中新增的链接跳转,并加入到釆集的链接池中,通过不断的寻找

新的链接地址直到将该网站的所有页面采集完整才结束退出。整个互联网可以

被模拟成一个大型的网站,通过上面的采集思想,在理想条件下可以将互联网

中的所有网页都采集下来。

而本系统使用的网络爬虫,完全有自己的风格:首先它并不抓取整个互联

网内容,即使是数据的来源网站也不抓取其整个网站,该系统的爬虫程序可以

自定义数据来源范围,根据需求添加、删除数据来源。以此,一方面可以减少

机器的资源消耗,另一方面,也圈定了一定范围内的数据,避免分析过多的无

用数据。

还一个特点是本系统的网络爬虫自带一级数据抽取的能力,可更具需求保

存需要的数据,该目的可以减少数据存储量。XPath即为XML路径语言选择器,

利用源数据的树形结构特点,采用路径跟踪的方式,定位目标数据的位置,主

要包含以下几个概念:

1、轴描述(用最直接的方式接近目标节点)

i-

2、节点测试(用于蹄选节点位置和名称)

3、节点描述(用于蹄选节点的属性和子节点特征)

2.8存储环境介绍

2,8,1关系数据库Oracle

1、特点

(1)完整的数据管理功能

作为一个商业化通用的存储系统,oracle有一套完整的数据管理功能:包括

存储大量的数据,其存储量基本满足目前大部分企业应用程序的业务数据存储;

数据保存的持久性,在硬件正常运行的条件下,数据可永久存储在库环境中,

直到用户手动删除;oracel有一套严格的数据存储校验规则,能够很好的保证

数据的准确可靠性;不同的oracle数据库之间可设置数据共享,从而满足一些

13
	基于网络爬虫的信息采集分类系统设计与实现
	




特定的业务需求。

(2)完备关系的产品

Oracle是一组成熟的关系数据库产品,其具备一整套完备关系准则,其中

包含信息准则,关系型DBMS的所有信息都遵循统一的信息准则;保证访问准

贝IJ;视图更新准则,只要视图相关表中数据发生了变化,视图结果数据也同时

变化;数据物理性和逻辑性独立准则,以此可以让数据管理逻辑变得更加简单。

(3)分布式处理功能

从第五版起Oracle数据库就提供了分布式处理功能,而到了第七版,分布

式处理功能有了一个比较成熟完善的体系,一个Oracle分布式数据库由

sqPNet、Oraderdbms、SQL*CONNECT 以及一些其他的非 ORACLE 的关系型

产品构成。

2、优点

Oracle具有以下几大优点:第一 放性,Oracle能在所有主流平台上运行,

同时完全符合所有的工业标准,完全开放策略使客户可以根据自身的需求选择

最适合的解决方案,全力支持开发商。第二安全性,Oracle获得最高认证级别

的ISO标准认证,而其数据安全的校验,容灾、数据恢复等功能都具备很好的

数据安全性保障。第三高性能,Oracle是windowsNT下的TPC-D和TPC-C的

世界记录的保持者。第四支撑各种应用模式,可以快速利用JDBC,ODBC, OCI

等网络客户进行连接,支持多种工业标准。第五使用风险低,Orade经过了长

期的各个版本发展,一直釆用向下兼容模式,并且得到广泛的应用,实践表明

使用Oracle完全没有风险。

3、存储结构

数据库模式对象和至少一个表空间共同组成其存储结构。模式对象是直接

引用数据库数据的逻辑结构,而模式则是对象的集合。模式对象包括这样一些

结构:表、视图、序列、存储过程、索引、同义词、簇和数据库链等。表空间、

段和范围构成逻辑存储结构,以此来描述如何使用数据库的物理空间。而其数

据库的关系设计由模式对象和关系形成。

数据库进行10操作的最小单位是数据块(Block),它不同于操作系统的

块。oracle数据库请求数据并不是以操作系统的块为单位,而是采用多个Oracle

14
	第二章相关技术简介
	




数据库块为单位。

段(Segment)是表空间中一个指定类型的逻辑存储结构,一个或多个范围

组成了段,段将占用并增长存储空间。其中包括:

(1)用来存放表数据的数据段;

(2)用来存放表索引的索引段;

0)用来存放中间结果的临时段;

(4)用于出现异常时,恢复事务的滚段。

范围(Extent)是数据库存储空间分配的逻辑单位,范围是由许多连续的数

据块组成,段则依此分配范围,首先分配的第一个范围称为初始范围,后续分

配的范围称为增量范围。

2.8.2分布式存储Hbase

在Hadoop生态系统中HBase是一个重要的NOSQL存储系统,它一些设计

思想来自于Google的Bigtable,在Key-Value存储结构上与Cassandra有很多相

似之处。

HBase出现的背景

进入数据时代,数据规模越来越大,很多业务场景需要很大的存储系统进	、

行支持,同时又需要存储系统可以动态的增加/删除,而绝大多数的关系数据库

还是专注于单台机器的存储能力,单台机器存储能力有限,同时I/O传输存在

上限,无法满足大量数据并发读写的要求。海量数据量存储成为当前大数据业

务分析程序设计的瓶颈,此时需要类似HBase这样的分布式高效读写存储系统

的出现。

应用场景

与同时代的Cassandra, Mongodb相比,HBase是一个重量级的nosql存储系

统。在实际产线中,目前最大的Cassandra部署规模在400台左右,Mongodb

的实际应用则相对较少。其它类似于memcache,redis,只是存储数据量有限的

缓冲系统,不能算式nosql存储系统。

Cassandra虽然已经达到了 nosql的存储要求,但是它的扩展能力有限,随

着集群规模不断庞大,其缺少master节点将导致缓存命中率下降。而HBase已

经突破了这一点,从facebook的上千台HBase部署规模可以看出。HBase不但

15
	基于网络爬虫的信息采集分类系统设计与实现
	




满足海量存储需求,同时其还有一个重要的特点,数据读写时I/O变化幅度较

小,延迟性变化不大。

HBase是hadoop生态系统中的一个重要成员,它与hadoop的其他产品如

Map/Reduce等天然结合,这样的完全兼容系统优势是其他nosql环境无法比拟

的。在一些实际应用场景中,各个产品之间可以互相配合达到最优的处理方案,

如在使用Map/Reduce对海量数据进行全局排序时,完全可以直接使用HBase

库中的TotalOrderPartitioner包进行处理,而全局数据读取完全使用hadoop的管

理功能,节点自我本地读取,减少网络之间的传输,极大的提高了数据读取的

效率。

hadoop HDFS自动分发完成HBase分布式存储后数据分块的replication。

这一优势使HBase与hadoop平台能够很好的结合运行于一个平台之上。同一系

列的产品hadoop与HBase其中也存在着侧重点不同,HBase的读写延迟性相对

较低,同hadoop HDFS存储相比,HBASE更侧重于单条数据快速交互读写,

响应速度快;而HADOOP HDFS则更侧重于批量数据的读写,虽然延迟高,但

适合离线分析等高吞吐量的数据处理。

因此虽然HBase和hadoop都是能存储海量数据的nosql系统,但是对于不

同的业务场景也要有选择的使用:在应对不需要实时响应,同时期数据处理需

要操作全局等海量数据时,可利用hadoop进行批量的数据分析;而在需要面对

实时数据交互的业务中,则需要使用HBase低延时的特点。同时hadoop是以磁

盘文件的形式存储文件,这里在写数据时要考虑到文件数据追加等问题,而

HBase釆用类似表的存储结构,以表的方式管理数据,方便业务程序快速开发

读写数据的接口程序。因此,HBase的出现弥补了 hadoop生态系统的很多不足

之处,让整个hadoop系统更加完善。

HBase是依赖于hadoop HDFS生存的,所以部署整个环境是一个比较大的

工作,除了需要部署hadoop HDFS外还要用到zookeeper duster。

16
	第二章相关技术简介
	




Master

-^r
	^)oKeeper

z教聽
	c_

/□
 ,...
 ...1
-□?'
	△	r, '、、
 -n △
?
	,
	?
 ?*
 *
 ,

`:
	:
	"?-
 .?八?《.
 .''
 :

*
	、
	?`?. ?`

/
	:
	x'-
 -;??□... □
.广??	、
	/
	;
i
 ?
 .-?'
 ;

r	^ ^ ^
\ k k

\	Reqlonserver teqiionierver	ReqiOi^servet
'.
 ?-—
 `
 ?-*□




\
 .、
 i
 , `

□、
	.、
	丨
	z

、-
 ??..
 i

??..、、	、羞1

. HDFS

图2-1 hbase集群分布图

其中Zookeeper作用在于:

1、hbase	regionserver 在 zookeeper 上进行登 I己注册,通过 zookeeper 管理

regionserver的状态信息。

2、hmaster将管理整个集群系统,通过维护系统表记录regionserver对应的

region信息变化,而regionserver则维护一个或多个region,hbase数据表的表

则是通过region完成。

2.8.3 key-vaiue 存储系统 Redis

Redis介绍

Redis是使用ANSIC语言编写、可基于内存亦可持久化的日志型、支持网

络、Key-Value开源数据库,并提供多种语言的API。Redis能支持大多数系统,

但目前官方的版本中还没有支持Windoxvs的。Redis目前更新到2.2.11,该版本

修复了一个2.2.7版本中遍历方式优化所出现的一个bug。

Redis的数据结构定义的相对灵活,主要集合一些实际业务经常用到的复合

数据结构,因此更加适合于业务场景的应用。除了基本的strings夕卜,Redis还

17
	基于网络爬虫的信息采集分类系统设计与实现
	




包含lists、hashes、sorted sets和sets等结构。通过这些数据结构的自由组合扩

展可以满足绝大多数的业务需求。

Redis的将所有的数据都保存在内存中,因此数据的读取响应速度和程序内

存一样,但也因此使得Redis的存储容量受到了限制,同时内存的数据在计算

机重启之后将会消失。为解决上诉问题,Redis提供日志式数据持久化,将数据

以日志的形式记录到磁盘上,以保障数据稳定,同时提升一定的数据容量。Redis

的优劣特效使其只符合一些特定的业务场景的需求。下面简述Redis的一些特

点:

1、数据类型

作为Key-value型数据库,Redis釆用了键(Key)和键值(Value)的一一对应关

系。除了使用数值或字符串,键值也可以是以下形式之一:

(1)	Lists (列表)

(2)	Sets (集合)

(3)	Sorted sets (有序集合)

(4)	Hashes (哈希表)

Redis通过键值的数据类型对该键值的数据进行操作。Redis可以支持列表、

集合或有序集合的并集、交集、查集等操作;另外,如果键值是普通数字类型,

Redis还可以提供自增操作等。

2、持久化

Redis数据都存储于内存环境中,为保障数据持久化,其提供两种方式对数

据进行持久化存储:一种是使用截图的方式,即将内存数据实时写道磁盘中,

这样不会影响业务数据操作,性能较高,但同时可能导致数据的丢失;另一种

釆用日志记录的方式,将每次的数据操作日志进行记录,保证每一次的数据修

改被记录,使数据完整一致,同时由于使业务数据操作多出了额外的工作量,

因此会影响系统的整个性能。

3、主从同步

Redis支持将数据同步,可以将本地服务中的数据同步到多台其他服务中。

4、性能

Redis的数据存储于内存中,使其读写性能达到了一个很高的程度。读写操

18
	第二章相关技术简介
	




作之间有显著的性能差异。

2.9本章小结

本章主要介绍了基于网络爬虫的信息釆集分类系统所涉及的相关技术,分

别包括java语言基础,J2EE的架构,web程序 发框架SSH,自然语言分析索

引查询技术hicene,网络爬虫技术,以及分布式存储、关系存储可key-value高

速存储等核心技术。通过对这些技术调研理解,能够更好的完成整个系统的设

计和实现。

19
	基于网络爬虫的信息采集分类系统设计与实现
	




第三章需求分析

3.1可行性分析

本系统主要结合网络爬虫,大数据存储,以及文本分析三方面主要技术设

计实现对互联网数据的获取、分析、挖掘。即系统可行性主要基于这三个方面

的技术支撑,下面介绍这三方面技术的应用可行性。

1、网络爬虫数据采集可行性

网络爬虫技术,已经在互联网搜索引擎中成功应用,该技术已经成功运用

于生产运营。本系统釆用网络爬虫核心釆集思想,主要针对实际业务需求进行

定制改造,改造方向为实现定向数据网络采集,针对特定的数据做可控配置釆

集,该技术的实现有很好的网络爬虫基础技术支撑,其具有较好的可行性。

2、文本大数据存储可行性

由于本系统需要采集大量的文本数据,同时为满足网络爬虫釆集效率的需

求,传统的关系型数据库将不适合这种业务场景,为此本系统采用基于Hadoop

的Hbase分布式存储系统。Hbase目前已成功在淘宝电商大数据等领域实现生

产应用,其具有很好的存储效率,同时能够动态扩展分布式集群的大小,完全

符合大数据存储需求。该技术很好的支撑了系统的数据存储。

3、文本分析可行性

文本分析主要包含文本分词,构建索引,文本聚类。本系统釆用中文IK

Analyzer分词器,IKAnalyzer是一个开源的,基于java语言开发的轻量级的中

文分词工具包。到目前为止,IK Analyzer—共推出了 4个大版本。从最初的

以Luence为应用主体的开源项目,慢慢的结合文法分析算法和词典分词的中文

分词组件。到了 3.0版本,IK已经发展成为面向Java的公用分词组件,完全独

立于Lucene项目,对Lucene的默认优化也提供了实现。在2012版本中,简单

的分词歧义排除算法的实现,标志着IK分词器已经不是单纯的词典分词,同时

在向模拟语义分词衍化。该分词器能很好的支撑本系统的文本分词;文本索引

构建釆用Lucene技术,目前大部分的全文索引需求都釆用Lucene实现,例如

apache软件基金会的网站的全文检索的引擎即釆用了 Lucene,IBM的开源软件

eclipse的2.1版本中的帮助子系统的全文索引引擎也釆用了 Lucene,IBM的商

业软件Web Sphere中釆用的搜索引擎也是Lucene;而文本聚类,目前已经有

20
	第三章需求分析
	




一些相对较好的算法模型可以支撑,例如布尔模型、空间向量模型等计算文本

相关度的算法,当然这方面还有很大的空间去研发新的算法,同时文本分类算

法需要依据一定的业务需求制定合适的算法。

3.2功能需求分析

3.2.1爬虫业务需求分析

1、应对复杂的各种网页结构

在面对互联网各种各样的数据结构时,我们需要支持种页面类型,新闻、

论坛、博客,图片、微博等,通过浏览器能看到的结构化内容。可以想象互联

网上的网页数据结构,可以说千万种,如果说一种网页结构就釆用一种代码逻

辑,那这将耗费巨大的成本在程序的开发上,显然是很不现实的。为此我们需

要设计一个易扩展、髙效率、适应各种网页结构的釆集逻辑。

2、区分任务优先级

互联网上的数据是复杂多样的,即使是其中的某一行业,其数据量也非常

大,所以在设计爬虫时,需要能够按用户的需要,区分不同数据源釆集任务的

优先级,以便利用有限的资源,快速获取有效的数据。在设计时,数据源任务

能够在不同的优先级间按需求自由更换,提高数据采集的灵活性。
	、


3、快速生成任务

网站页面成千上万,而爬虫的数据源任务是靠人工进行配置的,目的是快

速釆集需要的有效数据,以达到定向釆集的目的。这里需要控制数据源任务入

口位置问题,如果任务入口过于细分,如一个页面一个任务的话,将导致任务

量急剧上升,人工工作量也相应变大庞大;如果任务入口过于泛泛,如以网站

首页入口,这将导致釆集大量的无效垃圾数据,无端增加系统的计算量,泡是

不可取的。为此,数据源任务配置需分频道进行,在互联网的网页结果中,通

常将某一分类的信息归结到一个频道中,针对我们的业务而言,这是数据的第

一层分类,而且不需要消耗任何系统资源,所以可以充分利用网页的这一特点,

以频道快速生成数据源任务。

4、内容清洗

系统釆集的网页,其中包含各种各样的数据,用于内容呈现的网页代码占

据了 90%的数据量,而真正有效数据却只是其中很少的一部分。所以需要对釆

21
	基于网络爬虫的信息采集分类系统设计与实现
	




集到的页面数据进行清洗,过滤无关的垃圾数据。

5、数据格式化

网络爬虫数据釆集,其功能不仅仅只是将数据采集到本地的存储系统,为

了便于分析,对数据结构进行标准格式化,是其必备需求之一。互联网数据结

构复杂多样,通过数据采集将数据标准化为统一的存储结构,利于数据分析的

快速高效处理,为此需设计一条符合业务需求同时兼顾各种复杂网页数据结构

的存储数据字典,保证数据准确存储。

6、高速数据交换存储介质

根据该爬虫系统的初期设计需求,爬虫在采集数据时需要两大辅助数据的

支持,一部分是采集数据源任务,另一部分是数据抽取规则配置,由于网络数

据量是庞大的,导致这两部分的数据相应增加,将这些数据全部放到程序内存

中,势必导致内存溢出,但是这两个方面的数据又是实时将换的,如果将这两

部分数据放到外部存储,势必导致爬虫效率下降。为此需要一个高速数据交换

存储介质,该存储介质能快速读写,同时具有一定的存储空间。该系统使用以

KEY-VALUE为存储结构的redis,能很好的满足设备要求。

7、大数据存储介质。

本系统是对互联网大数据进行分类分析,所以设备上需支持大数据量的存

储。大数据存储并不仅仅指容量,数据存储需要很好的稳定性,读写高效性,

重要的一点是容灾可恢复性。据于上述需求采用基于hadoop的hbase存储机制

将很好 保证该需求。

3.2.2数据处理需求分析

1、文本分词

绝大部分的文本分析,其首要步骤都离不开分词,特别是中文分词。英文

以空格作为天然的分隔符,而中文继承了古代汉语的传统,词语之间并没有分

隔。古代汉语中除了人名地名以及连绵词等,其他词通常是单个汉字,所以以

前没有分词书写的必要。而现代汉语中双字或多字词居多,一个字不再等同于

一个词,因此首先要对一段文本信息进行分词。在分词中不仅仅将文本句子做

简单的词语分割,还需要考虑到字前后搭配得出的不同词语,这需要使用IK智

能分词接口,例如:“服装饰品有限公司”,分词结果为“服装I装饰I装饰品I饰品

22
	第三章需求分析
	




丨有限丨公司”。另一方面,分词词库中需加入业务特定行业的专有名词,以避免

专有名词被切分。

2、文本聚类

文本聚类是数据分析的业务核心,在设计这一部分逻辑算法的时候,需要

充分考虑业务需求:网络信息通常会有一部分持续热门出现的信息,例如股票

信息,相关各个行业很容易涉及股票,所以在文本聚类之前需要先过滤这一部

分的数据;在文本分类过程中,系统自身会定义出各个分类信息的规则,如包

含一些关键字,不包含一些关键字,然后通过分词关键字的数量统计,得出该

文本信息与各大分类的关联度,取关联度达到一定值的分类与该文本关联存储。

由于文本分类的自由性,这其中一篇文本信息可能会与多个分类都有关联,则

需关联上每一个分类。

3、文本索引
	?

文本匹配查询时,如果利用传统的数据库like模式,或者程序过滤匹配模

式,其在面对大量数据时,查询性能将急剧下降,导致系统性能降低,资源大

量耗费,用户体验效果很差。由于文本数据查询的必要性,需要对所有的文本

信息建索引,通过lucene倒排索引技术,建立索引文件,通过查询索引文件,

能快速定位到指定匹配的文本信息。

3.2.3数据展示需求分析

1、全文索引
	,

用户在查看数据时,需提供全文检索查询。通过用户输入所关心的关键字,

快速查询得到相关联的文本信息列表。

2、分类展示

数据分类为大小分类二级菜单展示,所有的信息都是与最小分类进行关联

的,所以在用户选择大分类菜单时,需展示出该大分类下所有小分类相关联的

文本信息列表,而选择某一小分类时,只显示该小分类下的文本信息。这里是

最终的业务需要达到的效果,即信息分类。

3.3性能需求分析

3.3.1数据采集效率

爬虫釆集都需要达到一定的效率才能满足业务需求。在采集系统工作过程

23
	基于网络爬虫的信息采集分类系统设计与实现
	




中,网站的响应速度直接影响着数据的获取速度,所以在选择目标数据源时,

要选择具有较好网络资源的网站,同时系统自身要具备自我调控能力,对于长

时间不响应的网站任务予以丢弃,不处理,保证其他数据及时获取。另一方面

网页数据抽取也是很大性能消耗,设计合理的算法才能保证系统快速高效解析

网页数据。

表3.1是本釆集系统的性能指标要求:

表3.1采集性能指标

性能指标项
	性能指标值(毫秒)
	峰值时间(毫秒)

任务加载耗时
	<10

网页获取耗时
	<500(网站服务器正常)	<5000

信息抽取耗时
	<20
	<100

信息保存耗时
	<5
	^10

模板加载耗时
	<1
	<10

系统各组件响应耗时
	<1
	<10

3.3.2数据分类效果

文本数据分类效果需达到两个方面的要求,第一是处理的速度,每篇文档

针对某个分类的处理耗时需在毫秒级,另一方面,数据分类需要达到一定的准

确性,大分类下分类误差不能大于20%,小分类下的分类误差不能大于5%。

互联网信息带有很大的随意性,来源数据的质量也会导致分类效果的准确度下

降,所以需要不断调整文本过滤、聚类算法,达到预计的效果。

3.4其他需求

1、系统的可靠性是信息系统应具有的重要特征,主要是指系统的不间断

运行的,同时可以满足业务处理响应时限要求的能力。在软件系统遇到非法数

据输入、相关软件的缺陷,或无法预料的操作情况时,系统都能从故障环境中

自然恢复过来并继续正确运行的能力。

2、系统应具备高度可扩展性,因为这一业务还不是特别成熟,往后会有

很大的变动及扩展,所以要求系统能很好进行延续,如数据源多样化,分类规

24
	第三章需求分析
	




则扩展化,同时具备开发新需求的灵活高效性。

3、运维简易性,系统由三大分布式系统组成,分布式釆集,分布式存储,

分布式分析,系统环境相对复杂,运维管理任务相对复杂而繁重,所以在各个

系统中需要好相应的日志级别输出,以便运维时,能够准确掌握整个系统的运

行状态,出现意外情况可快速定位问题。

3.5本章小结

本章主要介绍系统的功能需求,包含爬虫业务信息釆集需求,数据分析需

求,以及数据展示需求,在爬虫定向爬取的需求中做深入的分析。另外由于数

据量的原因,系统整体的性能需求也做了详细的分析,包括信息采集效率以及

数据分析的效果等。其他非功能性的需求包括系统整体稳定性以及后期研发的

可扩展性和运维简易性需求的分析。

25
	基于网络爬虫的信息采集分类系统设计与实现
	




第四章系统设计

4.1系统框架设计

该系统的总体框架设计为:分布式平台将分布式流计算框架storm、分布式

存储hbase以及高速读写redis集成为一个统一 发平台。在该平台上 发采集

系统,交由分布式平台storm运行采集数据,将结果数据存储于hbase中,其次

开发文本聚类业务逻辑代码,同理交由storm分布式环境进行计算,最终将聚

类结果信息存储于oracle关系数据库中,并对文档建立索引;另外构建两个web

应用,一个是为实现采集系统的可配置化的配置界面,另一个是展示最终分类

结果数据的展示系统,这样就构成整个系统的框架模型。

分类结果web展示

>"□) r、
?
	”乂*"W”	?-美?????

I I
	f	p ;
i , ^
	I索引文件 ;
;I Orcaie
	I
	!

j^ I I
I C55Z)丨“
^
	,	=

:
 /"\ 	 i 一

:广一“'—~17~
	叫. [采集系统配置	i f
i、::纖系! :、;; ^ Redis	I =	i i

i
	、.一.乂〗
	i

If

士 :..JiKr 网 p

图4-1 :系统架构图

26
	第四章系统设计
	




4.2爬虫设计

\ P人列管理七T任务管理】板管理,口他参数配贾丨
;4'' 一L__?J X y :: 一 I」
jj.I.`□*□' ` MM"""? II ` _丨_”-<w> □□□ mm □ ^iwg—'^w^aiww

爬虫配s界面-^r'
, -:.4 `/ t

"1 KtKWaWlKSJSfcewWWWaiRM
	:? □ g^W,;--!-''-- :?---
				 	
	MS5W

? □
….

,aiMIIMMl—iPi—ill_ll'-' □
:
==^ I P=--

、V,

读取任务*□`、f、

|E

获取网页(.: 虫

组

休-:

数据抽取-:

数据保存〃

.m
	`

图4-2爬虫功能模块图

1、添加数据采集任务队列

用户进入采集系统,为分类不同的采集规则,添加数据采集任务队列

主事件流:用户通过按键输入队列釆集规则,如起始任务,时间规则,队列

id等,按“保存”按钮进行数据提交。系统验证数据是否完整正确。若数据有

误,系统提示相应数据输入有误;若数据完整正确,保存该釆集任务队列信息,

该用例结束。

异常事件流:用户按“保存”按钮进行提交时,若用户输入数据不完整,系

27
	基于网络爬虫的信息采集分类系统设计与实现
	




统进行提示,并重新输入,用例重新开始。

2、修改数据采集任务队列

用户进入釆集系统,为改变队列采集规则,修改数据釆集任务队列

主事件流:用户通过点击“修改”按钮,打 该任务队列历史配置信息,

经过按键编辑修改队列配置信息,按“保存”按钮进行数据提交。系统验证数

据是否完整正确。若数据有误,系统提示相应数据输入有误;若数据完整正确,

保存该采集任务队列信息,该用例结束。

异常事件流:用户按“保存”按钮进行提交时,若用户输入数据不完整,

系统进行提示,并重新输入,用例重新开始。

3、删除数据釆集任务队列

用户进入采集系统,为取消队列采集规则,删除数据釆集任务队列

主事件流:用户通过点击“删除”按钮,弹窗提示是否确定要删除该任务

队列信息,点击“确定”,删除该队列信息,用例结束;点击“取消”,返回页

面,不做删除,用例结束。

4、添加数据采集任务

用户进入采集系统,为扩展数据源,添加数据釆集任务

主事件流:用户通过按键输入关键字链接等信息,按“保存”按钮进行数

据提交。系统验证数据是否完整正确。若数据有误,系统提示相应数据输入有

误;若数据完整正确,保存该釆集任务信息,该用例结束。

异常事件流:用户按“保存”按钮进行提交时,若用户输入数据不完整,

系统进行提示,并重新输入,用例重新开始。

5、修改数据采集任务

用户进釆集该系统,为改变数据釆集规则等,修改数据釆集任务

主事件流:用户通过点击“修改”按钮,打开该任务历史配置信息,经过按

键编辑修改队列配置信息,按“保存”按钮进行数据提交。系统验证数据是否

完整正确。若数据有误,系统提示相应数据输入有误;若数据完整正确,保存

该釆集任务信息,该用例结束。

异常事件流:用户按“保存”按钮进行提交时,若用户输入数据不完整,

系统进行提示,并重新输入,用例重新开始。

28
	第四章系统设计
	




6、删除数据釆集任务

用户进入采集系统,为取消某数据源,删除数据采集任务

主事件流:用户通过点击“删除”按钮,弹窗提示是否确定要删除该任务

信息,点击“确定”,删除该任务信息,用例结束;点击“取消”,返回页面,

不做删除,用例结束。

7、移动数据釆集任务

用户进入釆集系统,为改变任务所处队列等,移动数据采集任务

主事件流:用户通过勾选要移动队列的任务,点击移动队列,弹窗提示队

列选择,选择完队列点击“确定”,将勾选的任务移动到所选择的任务队列中,

用例结束。

8、添加相关釆集任务的数据抽取规则

用户进入釆集系统,配置相关采集任务的数据抽取规则

主事件流:用户通过按键编辑xml格式的抽取规则,按"保存”.按钮进行

数据提交。系统验证数据是否完整正确。若数据有误,系统提示相应数据输入

有误;若数据完整正确,保存该数据抽取规则信息,该用例结束。

异常事件流:用户按“保存”按钮进行提交时,若用户输入数据不完整,

系统进行提示,并重新输入,用例重新开始。
	、

9、修改相关采集任务的数据抽取规则

V'


用户进采集该系统,为改变数据采集规则等,修改数据采集任务

主事件流:用户通过点击“修改”按钮,打开该抽取规则历史配置信息,

经过按键编辑修改规则信息,按“保存”按钮进行数据提交。系统验证数据是

否完整正确。若数据有误,系统提示相应数据输入有误;若数据完整正确,保

存该采集任务信息,该用例结束。

异常事件流:用户按“保存”按钮进行提交时,若用户输入数据不完整,系

统进行提示,并重新输入,用例重新开始。

10、删除相关采集任务的数据抽取规则

用户登录釆集系统后,为取消某数据釆集规则,删除相关采集任务的数据抽

取规则

主事件流:用户通过点击“删除”按钮,弹窗提示是否确定要删除该规则信

29
	基于网络爬虫的信息采集分类系统设计与实现
	




息,点击“确定”,删除该规则信息,用例结束;点击“取消”,返回页面,不

做删除,用例结束。

11、任务调度

根据配置的优先级以及触发事件规则,调度任务执行优先顺序。

主事件流:获取所有的任务规则队列,根据配置信息触发队列,根据队列

优先级获取最高优先级队列,从重顺序分发采集任务。

12、釆集数据

获取任务,根据任务配置釆集数据

主事件流:接收任务,根据任务配置获取对应链接地址的网页信息,对信

息进行简单过滤标准化处理,递交到数据抽取组件。

13、过滤抽取数据

清洗数据,格式化数据结构

主事件流:接收标准化的数据,读取模板规则,根据模板配置信息,定向

抽取数据,同时格式化数据结构,递交到数据存储组件。

14、数据保存

主事件流:接收格式化后的数据,保存标准数据到存储环境hbase中。

4.3数据分类逻辑设计

1、文档聚类

对釆集的数据进行按设定的条件,分级归类

主事件流:获取釆集后的文档,对文档进行初步过滤,主要去除网络热词产

生的垃圾数据,其次根据设置的关键字分级条件,对文档进行分析处理,将文

档归类到相应的目录之下。

异常事件流:异常文档,数据不完整,直接丢弃该文档。

2、创建索引

对采集的文档数据构建全文索引,方便快速搜索。

主事件流:获取采集后的文档,对文档标题,内容进行分词,构建hmcene

倒排索引。

异常事件流:索引创建异常,报错误日志,继续处理下一篇文档。

30
	第四章系统设计
	




4.4 Web展示设计

1、全文搜索

用户进入分类展示系统,为搜索相关数据,可使用全文检索框

主事件流:用户通过按键在全文检索框中输入关键字,按“搜索”按钮进行

数据提交。系统根据所提交的关键词,检索出相应的文档信息,该用例结束。

若用户未输入关键字即点击“搜索”按钮,则提示用户需先输入关键字,用例

重新 始。

异常事件流:用户按“搜索”按钮进行提交时,若用户未输入关键字,系统

进行提示,并重新输入,用例重新开始。

2、一级菜单

用户进入分类展示系统,选择菜单查看相应分类数据

主事件流:用户通过点击一级菜单,系统将展示符合一级菜单下所有二级菜

单的数据。

3、二级菜单

用户进入分类展示系统,选择菜单查看相应分类数据

主事件流:用户通过点击二级菜单,系统将展示符合二级菜单的所有数据。

4.5数据结构设计

由数据实体的结构图,进一步进行Oracle数据库系统所支持的实际数据模

型的设计,具体描述数据库的逻辑结构。

4.5.1数据关系模型

根据需求分析和用例分析,该系统数据库将分为以下实体:

Oracle关系数据库:

1、菜单表CATES	(菜单编号、菜单名称、菜单等级、上级菜单、菜单类型、

菜单排位、包含与不包含关键词、是否操作、网站名称)

2、文本对象表INFO	(文档ID、文档标题、文档原始链接、文档网站、文

档类别、文档爬取时间、文档作者、文档、创建时间、是否境外、文档在HBASE

中的ID、文档命中词、文档类型)

3、参数控制表TR_CONTROL_PARAMS	(属性编码、属性名称、属性值、

是否生效、创建者、创建人、修改时间、属性类型、文本类型)

31
	基于网络爬虫的信息采集分类系统设计与实现
	




4、分词扩展词表TR—WORD_DICT (关键词、创建时间、创建人、操作类

型、批量数)

Redis釆集系统配置信息:

1、任务队列jobRun(任务队列id、名称、id、优先级、任务开始位置)

2、任务信息task	(连接地址、id、任务队列id、名称、模板id、……)

Hbase源数据存储:

1、文档表bg_source_document	(文档标题、文档来源、文档链接、发布时

间、网站名称、内容、摘要、爬取时间)

2、任务表bg_analyse—task	(文档标题、文档来源)

4.5.2实体一关系模型

系统的实体-关系(Entity-Relationship,ER)模型如图5-11所示

芙絲
	X本辦象衷

vmmam)""”<必	士档 ID
	"““<rk>

芙i名移
 vmmiim)
	文.档挣S
	vmmiam!

笑ii?绞
 S3BEI
	文挡.f始每接	VMOM2C512)

上级笑丘
 wmmim)
	文.档wnmium

菜 iilis
 YMMRIiW
	文档类到
	$AmM2tS2)

mm
	文挡/6?时间	MH

包含与不包含关蜜谓文档作老
	vAioai2_)

是杏赛作
 VMMK2(2?5)
	文□档fl尝射间	IMTiMSK^S)

,站名移
	是奋埃并
	⑵

vmsimm)

文趋金令 _ ?113112(512)

		
	 雅細
	fiTOtafa	




iiFlil
	参数□表

雜究vmmim
	雜麟碰猶却

□— MTE
	雜名?	%IOM2(25S)
i'lS人 B5Oai2(l0)
	'iftS	$iSGMffi(2S)
vimMS2_
	?效	,
批i 数 liSBSi
	fJ 套者	HV'AEOMS2(4)
tS建A	MTE

镰改时间BAT!

itHi	$M3M12{64)

	 文絲 g	IHKEl

图5-11数据库关系表图

32
	第四章系统设计
	




4.5.3数据结构

系统的数据字典如表4-1到表4-7所示

1、Redis数据结构

Redis的所有数据都是以keyValue的形式存储其主队列上,查询数据一般釆

用key或者key的前缀进行过滤。表4-1描述了爬虫采集配置的数据字典,其

存储在redis中,主要包含运行队列TaskRunQ为key, value是以队列为元素组

成的列表,该数据为发送指爬虫需要运行的队列信息;队列信息以jobRun*为

前缀,value是队列信息的一个Map,该数据为队列基础信息数据;任务队列以

Taskls为前缀,value为任务主键key组成的列表,该数据存储队列包含的任务;

任务信息以task为前缀,value是任务信息的一个Map,该数据存储单个任务的

基础数据。

	
	表4-1采集配置数据	
	




	Key
	描述
	Key
	描述

T
	^ , jobRun* (1)
	队歹!J信息一	



TaskRunQ	^灯队歹』 jobRun* (2)	队列信息二

Tasklsid
	任务队列id

Name
	名称
	




jobRun*
	队歹[J信息 Jd
	id
	




Priority
 优先级

	Index
	任务开始位置

Taskls*	任务队列 taskid (1)
	 絲 id二	




	 1 士分P人力	taskid (2)
	 任务 id 二

uxl
	连接地址	




Id
	— id

task*
	任务信息 Tasklsid
	任务队歹!J id	




Name
	名称
	




	pageType
	模板 id
	




2、Hbase数据结构

Hbase的数据以主键列族的形式存在,一个主键下可定义多个列族,这在建

表的时候必须声明,而一个列族下可任意扩展多个字段,并且字段名可以随意

取值。表4-2为存储采集源数据的文档表,包含文章标题,来源,链接,发布

时间,主题内容,采集时间等基本信息;表4-3为数据分析任务表,每采集一

条数据会在该表尾中产生一条任务数据,分析程序检测该表产生新数据,进而

33
	基于网络爬虫的信息采集分类系统设计与实现
	




读取源数据进行分析。

	 表 4-2 文档表(bg^source document)
	




Row id 
	—	
	
	




—
	名字
	说明
	名字
	说明

Title
	文档标题

Source
	文档来源

url
	文档链接

submit time	发布时间

MD5 (url) doc—info 文档信息 	—
	




—
	Websitename 网站名称

Content
 内容

description 摘要

	crawl-time	爬取时间

	表 4-3 任务表(bg^analyse task )
	




o^
	列族
	^
Row id
	
	j
	j
	




	:	 名字
	名字	说明

SYSDATEL0NG()+MD5( , . ^ 任务信 doc_rowid 文档 id
1、
	taskinfo	白	^ ; ;	日 八 +广~
url)
	-	恳	is_analysed 是否分析

3、Oracle关系数据库结构

相对于其他数据存储环境,关系数据库更加方便查询及数据统计等业务需

求,为此关于业务部分的数据存储在oracle关系数据库中。表4-4为分类数据

定义存储表,主要存储数据分类依据,其最终以菜单形式在界面上展示,固设

计成父子菜单模型,包含菜单名称(分类名称)、编号、上级菜单、包含与不包

含关键字等信息;表4-5为符合各分类的文章数据列表,包含文章各基础信息

字段,以及分类,其中文章的内容并不存于此处,而只是存储其在Hbase中得

主键,这是为了减少关系数据库的数据量。同时用主键查询hbase的数据性能

符合业务需求。表4-6为参数控制表,主要是对分析程序逻辑相关控制参数的

设置。

34
	第四章系统设计
	




		表 4-4 菜单表(CATE”			




名称
 代码
 数据类型	长度 主键 外键

菜单编号	CSJD	VARCHAR2(32)	32 TRUE	FALSE

菜单名称	CS_NAME	VARCHAR2(100)	10。 FALSE	FALSE

菜单等级	CS—LEVEL	NUMBER
	FALSE	FALSE

上级菜单	CS_PARENT VARCHAR2(32)	32	FALSE	FALSE

菜单类型	CS_T"YPE:	VARCHAR2(32)	32 FALSE	FALSE

菜单排位	CS—SEQ	NUMBER
	FALSE	FALSE

包含与不包 CS KEYWO
 1 FALSE FALSE

含关键词 RDS	 VARCHAR2(1)
	




是否操作 CS_ACnC)N VARCHAR2(1) ^	FALSE FALSE

_倾 CS_WEBSIT 霍函 _) 200 FALSE FALSE
E

		表4-5文本对象表(INFO)		
	




名称 I 代码 I 数据类型 I长度I主键I夕卜键

文档 ID	INFO ID	VARCHAR2(32) 32 TRUE FALSE	`

文档标题
	trVARCHAR2(1001000 FALSE FALSE“ -
INFO TITLE m

	I
	^
	




文档原始链接 INFOJURL VARCHAR2(512) ^12 FALSE FALSE

湖站『0_WEBSIT VARCHAR2(200) □ FALSE FALSE

文档类别 INFO—CATE VARCHAR2(32) 32 FALSE FALSE

文档爬取时间 INFO DATE DATE
	FALSE FALSE

細乍者INF0_AUTH0 VARCHAR2(200)FALSE FALSE

文档创建时间 iNFO-CDAre VARCHAR2(200) 200 FALSE FALSE

是否境外 INFOJSOUT VARCHAR2(2) 2 FALSE FALSE

文档在	TXTi^n nor^pn
	^FALSE FALSE

HBASE 中的	_
	VARCHAR2(50)

JP	 ?
	




文档命中词 INFO_HITVAL VARCHAR2(512)	FALSE FALSE
UE

文档类型 INro_WTYPE VARCHAR2(2) 2 FALSE FALSE

35
	基于网络爬虫的信息采集分类系统设计与实现
	




表 4-6 参数控制表(TR_CONTROL_PARAMS)

一名称 I 代码	数^类型 I长度I主键I外键
属性编码 PARAM—COD	…U ^ FALSE FALSE

b

属性名称 PARAM_NAM	VARCHAR2(256) 256 FALSE FALSE
E

属性值 PARAM VALU
	256 FALSE FALSE

—	vaRCHAR2(256)
E

是否生效 IS—VALID INTEGER
	FALSE FALSE

创建者 CREATOR	NVARCHAR2(64) 64 FALSE FALSE

创建人 CREATE TIMFALSE FALSE
^ — DATE

	E	

修改时间 UPDATE—TIMFALSE FALSE
UAlb

	E	

属性类型 PARAMJTYPE VARCHAR2(64) 64 FALSE FALSE

文本类型 DOC—TYPE INTEGER
	FALSE FALSE

表4-7分词扩展词表(TR_WORD_mCT)

一名称	代码	数据类型	I长度I主键I外键

关键词	KEYWORD	VARCHAR2(64)	64	FALSE	FALSE

创建时间	CREATEjriME	DATE
	FALSE	FALSE

创建人	CREATOR	VARCHAR2(10)	10	FALSE	FALSE

操作类型	OP_TYPE	VARCHAR2(10)	10	FALSE	FALSE

批量数	BATGH_NUM	NUMBER
	FALSE	FALSE

4.6本章小结

本章节主要是根据系统需求,对系统进行初步的设计,主要涵盖系统框架

设计,对系统架构,层次结构进行划分;爬虫系统设计,采集逻辑设计以及釆

集系统控制,任务调度规则,重拨爬取等的设计;web展示设计,爬虫系统控

制配置界面,数据展示结果查询web页面的事件流设计。通过上述三个层次系

统的设计,使整个系统模块更加清晰,方便下一步的系统实现。同时针对系统

业务需求进行各模块存储数据结构设计,完成数据字典的定义。

36
	第五章系统实现
	




第五章系统实现

5.1开发环境的搭建

1、下载	JDK1.6

安装 JDK: jdk-6u25-windows-i586.exe

2、下载并安装MyEclipse	10.0

安装 MyEclipse; myeclipse-10.0-offline-installer-windows.exe

3、下载并安装oracle

4、基于分布式开发平台(storm、hbase、redis集群构成)

5.2系统功能实现

5.2.1数据采集系统

采集系统主框架运用storm分布式流计算框架,该框架以topology为一个运

行体,topology由各个运行组件构成,所以釆用组件化的形式进行处理逻辑

的实现。

1、数据源任务调度组件

该采集系统为实现数据源的可扩展性,方便数据范围的圈定,需实现数据 ?

源任务的可配置性;同时根据各个任务的数据热度,将任务置于不同的釆集队

列中,配置相应队列的触发规则,釆集频率等参数,以控制整个采集系统的任

务调度。具备上述配置信息,即可实现任务调度组件的业务逻辑。

业务逻辑:首先,采集系统的配置平台可根据队列的触发规则,触发队列

任务,将队列配置信息发送到采集运行队列中,调度组件实时读取该运行队列,

当获取到队列信息后,根据队列的配置信息,相应读取数据源任务,获取数据

源任务之后,即组装采集系统的采集任务发送给釆集组件进行处理。分发采集

任务的同时,队列的配置信息相应的有所调整,以适应下一个任务的正确分发,

保证釆集系统正常运行。图5-1为任务调度逻辑图。

37
	基于网络爬虫的信息采集分类系统设计与实现		
	




一—
	1优先:競1
,队列哪;	(?)
	.	丨:A :|
□□-— ? 触发N
	:判断优先 .^
I	〉daiv
	银,	艰先—
广—、〈
	级.级岸.、任务
(队列配置2 )
	”	[—-!.> fx%

图5-1任务调度逻辑图

相关代码逻辑:

/**

*获取优先级最高的队列,没有队列返回null

*	@param taskMap

*	@retum String

*/

public static Map<String, String〉getFirstTask(String pool) {

查询队列jobs;

for (String job : jobs) {

查询队列详情joblnfo ;

int value = joblnfo.get(优先级)

if (value > max) {

ret 二 joblnfo;

max = value;

}

};

return ret;

}

pool参数定位redis数据库实例,运行队列中可能已经发送了很多个需要处

理的任务队列信息,该接口是根据各个队列配置的优先级,选择需要优先处理

的队列出来,该逻辑保证队列运行的先后顺序,配置中,可把热点数据置于高

优先级任务队列中。

38
	第五章系统实现
	




*获取任务

*	@param pool

*	@param job

?/

public static Map<String, String> getTaskMap(String pool,队列 job) {

}

pool参数定位redis数据库实例,quene即前一个接口获取到的需要优先运

行的任务队列配置信息,根据该配置信息,定位釆集任务的位置,返回第一个

需要运行的任务信息,相应修改quene的配置信息,下次处理该任务时,即处

理下一个任务的分发。

/**

*任务分发组件spout

**/

public class BaogangSpout extends BaseRichSpout {

优先级队列 job = getFirstTaskO;

任务 task = getTaskMap(job);

JSONObject json = JSONObject.fonnat(task);

//发送任务

spoutCollector.eniit(new Values(j son));

}

任务分发控制组件,该组件未storm框架的数据入口类,所有的处理任务都

是通过该组件即spout组件从外部数据库或其他数据源读取,分发到后续各个

处理组件中。在本釆集系统中,spout组件主要提供任务数据读入,同时控制任

务分发的逻辑和频率。

2、采集组件

系统釆用javahttpClient采集网页数据,根据链接地址获取网页源码,期间

要解决失败重抓,IP封锁,编码转换,源码格式化等问题。失败重抓为保证数

39
	基于网络爬虫的信息采集分类系统设计与实现
	




据尽可能完整正确,ip封锁解决服务端拒绝数据采集,编码转换是保证数据正

确有效,源码格式化利于数据的统一抽取。各阶段不可或缺的组成的采集组件

的核心内容。

业务逻辑:采集组件在接到上游组件的任务后,组装必要的参数控制,如

页数累计,链接格式化等,然后通过标准格式化处理过的链接地址,调用

httpdient获取该网页的源码,抽取出该源码的编码规则,若符合系统统一编码,

则不作处理,否则转换为统一编码规范,之后将编码正确的网页源码进行标准

格式化:补充丢失的标签,过滤javascript等脚本语言,以及ess等,最后将处

理好的网页源码交由下一组件(数据抽取组件)进一步处理。图5-2为业务逻

辑图。

彳壬务!据处^米%:,码

图5-2采集组件业务逻辑图

相关逻辑代码:

/**

*带adsl重拨抓取组件

*/

public class CrawlerAdsl extends CrawlerBase {

public html crawlerByAdsl(){

String html = crawler(task);// 釆集

while(html 封锁页面&&size<10){

重拨IP;

html = crawler(task);

Size++;

}

Return html;

}

40
	第五章系统实现
	




}

应对IP封锁的抓取控制类,该类有抓取控制参数,以及一个主处理方法完

成数据抓取,当出现ip封锁的情况下,可通知第三放服务(adsl池服务)做线

路切换和重拨等。保证数据采集的同时,避免被IP封锁的情况下数据无法获取,

导致相关数据缺失。

/**

*抓取客户端
	.

*/

public class My Client {

}

httpClient抓取网页的核心类,该类类似于一个微型浏览器,传入连接地址,

返回地址下的网页源码。主要处理httpclient浏览器标准,访问超时,压缩编码

规范等参数,其中上面的代码及处理压缩编码的网页源码,即将与本系统不一

致的编码转换为统一编码规范。

/**

* 文档 crawlerBotl

*/

Public class BaogangDocumentCrawlerBolt extends BaseRichBolt{

}

该组件为storm中的任务处理组件Bolt,由于数据抽取与抓取紧密连接,所

以在本组件除包含抓取的业务逻辑调用外,还包含了数据抽取的业务调用。在

完成数据抓取之后,组件中直接调用数据抽取工具类,抽取的结果数据发送给

后续的数据存储组件,产生新的抓取任务,发送给本组件处理或者后续任务处

理组件。

3、数据抽取组件

基于硬件资源的考虑,在没有大集群的环境下,过多的无用数据,只会导

致浪费太多的存储空间,为止,我们要将釆集后的数据做一定的抽取,只提取

有价值的信息,一方面可以减少数据存储量,另一方面,对数据有一个标准化

的存储,方面后续的数据分析。

41
	棊于网络爬虫的信息釆集分类系统设计与实现
	




业务逻辑:抽取组件主要利用 源简易公式为桥梁,将散乱的结果数据根

据公式规则,转化为标准的结果数据。简易公式中有一个数据容器,将分页信

息,翻页信息,源码信息,任务信息等全部添加到数据容器中,公式中的函数

通过相应的变量,定位目标数据,进行数据抽取和组装,所以这里需要一个配

置数据抽取规则的模板,通过该模板可以很灵活的进行数据抽取。图5-3为业

务逻辑图。

(""采集结果"^[^>扣乂漂入数	,	—
A]	数据抽取,—I .结￡|^
z-—、
		组装结果匸^, V 果

集任务取兵取校板

图5-3数据抽取业务逻辑图

相关代码:

/* *

*解析主处理

*	@param pageType

*	@param variables

*	@param nextTasks

*/

public HashMap<String, List<HashMap<String, String〉〉〉semanticPage

(String pageType, List<Variable> variables,List<JSONObject> nextTasks) throws

Exception {

//获取模板规则

List<String> myrules = getTemplate(pageType);

//循环模板规则

for (String myrule : myrules) {

variables数据容器

公式 expression = JSONObejct.fromat(myrule).get("expression “)

42
	第五章系统实现
	




value = ExpressioiiEvaluator.evaluate(expression, variables);

result.put(field, value);

}

return result;

}

该方法为数据抽取主处理方法,参数pageType为模板规则id,通过pageType

找到数据抽取的规则模板,variables即数据容器,nextTasks是翻页任务集合。

该方法主要实现读取数据抽取模板,根据模板规则抽取相关数据,组装结果,

对结果数据进行完整性判断,对新产生的人进行重复性检查,最后确定最终的

结果数据。

!**

*数据抽取公式自定义函数

*/

public class IKFunction {

}

该类定义了相当数量的公式自定义函数,数据的抽取是灵活多变的,可以

根据实际的需求,自定义相关抽取逻辑,即在该类中实现代码逻辑。目前已包

含xpath数据抽取,jsoup过滤器,数组、字符串、以及json数据等的相关处理

函数,这些函数可以在公式中任意组合嵌套,完成数据的准确抽取。

/**

*	html->tidyDom 转化

*	@parain html

*/

public static Document parseDom(String s) throws

ParserConfigurationException, SAXException, lOException {

org.dom4j .Document doc = null;

try {

doc = DocumentHelper.parseText(s);

} catch (DocumentException e) {

e.printStackTraceQ;

43
	基于网络爬虫的信息釆集分类系统设计与实现
	




}

java.io.StringReader reader 二 nev/java.io.StringReader(doc.asXML());

org.xml.sax.InputSource source = new org.xml.sax.InputSource(reader);

javax.xml.parsers.DocumentBuilderFactory dbf =

javax.xml.parsers.DocumentBuilderFactory.newInstanceO;

javax.xml.parsers.DocumentBuilder documentBuilder =

dbf.newDocumentBuilderO;

return (documentBuilder.parse(source));

}

该方法将字符串形式的html源码标准化处理成Document的格式,即按网

页标签转换为树状结构,方面与利用xpath进行数据查找定位。

4、数据保存组件

数据保存组件也是storm中的一个任务处理组件Bolt,不过这里实现的逻辑

仅仅是将上游组件传递过来的结果数据,分别保存到各自的结果表中,这里采

用hbase作为数据存储库。

业务逻辑:上游传递过来的结果数据包含了其存储的目标表名,以及相应

的数据记录列表,根据表名,组装存储数据,调用存储接口保存数据。若需要

验证数据已存在不再更新,则在存储前检查hbase库是否已存在该记录。图5-4

为数据保存业务逻辑图。

存储数掘

图5-4数据保存业务逻辑图

相关代码:

/**

*	Hbase 操作 Api

*

*/

public class HBaseApi{

44
	第五章系统实现
	




public void save(格式化结果列表 result){

for(result){

保存

}

}

public void query(条件 filter) {

}

public void query(条件 filter) {

}

}

Hbase操作接口,定义hbase相关的查询、保存、删除等接口,实现数据操

作与业务逻辑的独立。

5.2.2数据分类
	t

1、文档分类

文档数据分类,即根据文章中的词汇情况分别情况进行归类。所以在定义

分类是需具体设置文章必须包含关键字,必须不包含关键字,所属网站等等相

关信息,以提供程序逻辑判断的依据。归类条件可视业务情况进行定义,本系

统基于钢材材料的文档分类,单以关键字的分布情况进行分类,分类中定义包

含及不包含关键字,以此决定文档的归属。

业务逻辑:首先第一步做简单砂布式过滤,将一些搜索热词导致的垃圾数

据过滤掉,如股票信息等;第二步根据信息发布网站判定,若归类中定义了网

站名称,而该信息又是发布于该网站,则该文章属于本类别;第三步过滤必须

不包含关键字,若文章中含有必须不包含关键字,则丢弃该文档;第四步判定

是否含有包含关键字,若有加入分类,若没有丢弃文档,继续分析下一个文档。

以此循环不断的处理一个个文档,将文档归类到符合条件的类目之下,包括父

子类目间的关联。图5-5为文档分类业务逻辑图:

45
	基于网络爬虫的信息釆集分类系统设计与实现
	




一山nr?r: 之

_ false
		




(it.
		1 tme
\ & m-Mm J
	'..

4r

_ iMMiM* tmmm mwwiii mmmmi nmmm mmmm
	mmmm tamim mmm wmmm mmmm iiimmmtw nwniium

,Z
	丨,,	、

I
	)
? ^ ^ 1
I
	I

I
	I

,
	—II —
	I
I
	Z'\
	a
,
	false
	I	-□;;^—_I
	i
i
	、、、、z-z
塞,不約入统、\
 `
	I
I [ tK
	——.么
	




乂嵌殖索"31 7
	z
	_
i V.__乂
	^
	I
囊
	t^l'-
	fcaJMil
	讓

I
	JBF
	I

I 广J
/ .. 一…..\ \
	Fnlse
	?
1 f .父n。	1
	
	,
\ Doelnfo /
	--
	_
I v_y
	I
I
	f 孩/-节 \
	I
I V Docinfe) : ::: j
	*
^
	vzzy
	f
\ ^z
^□WMMaat MMMM MMMm mmmm □—lum* mmmm ombi mmmm mmmt tmmm mmmmt ihhhwi mmmm ccMOir

图5-5文档分类业务逻辑图

相关代码:

if(content不包含“股票”){

for(分类列表){

if(来源网站不属于配置网站列表){

forC必须包含关键字列表){

if(存在){

continue;

}else{

break;

}

}

}else{

46
	第五章系统实现
	




直接加入该分类

}

for(必须不包含关键字列表M

if(存在){

break;

}else{

continue;

}

}

if(满足必须包含和必须不包含条件){

计算包含关键字指标判断是否归属该分类

}

}

}

这是一段伪代码,即文档分类的核心逻辑,主要在发布网站,必须包含关

键字,包含关键字和必须不包含关键字等条件下做判断,决定文档的分类归属,

或丢弃该文档。

/**

*分类实体对象

*/

public class CsType {

private String csid;

private String csName;

private int csLevel;

private String csPraent;

private String csType

private int csAction;

private int csSeq;

47
	基于网络爬虫的信息釆集分类系统设计与实现
	




private List<String> csKeywords;

private List<String> csKeywordsNot;

private List<String> csKeywordsMust;

private List<String> csWebSiteName;

private List<Info> infos;

}

分类信息对象,对象中主要几个成员变量有,包含关键字csKeywords,必

须包含关键字csKeywordsMust,必须不包含关键字csKeywordsNot,发布网站

csWebSiteName,以及属于该分类的文档列表infos。

/**

*文档实体对象

*/

public class Info {

public String infold;

public String infoDocRowid;

public String infoTitle;

public String infoUrl;

public String infoWebsite;

public String infoContent;

public String infoDate;

public String infoAuthor;

}

文档信息对象,对象中主要几个成员变量有,包含文档任务Id infold,文档

Id infoDocRowid,标题 infoTitle,链接地址 infoUrl,网站 info Website,内容

infoContent,发布时间 infoDate,作者 info Author。

2、创建文档索引

构建文档索引,这是文档检索中必不可少的一个步骤,主要利用lucene及

中文分词器IK,将文档内容进行中文切词,对切词后的每个词语在文档中出现

的位序进行倒排索引的创建,使页面在搜索框搜索的词语快速定位到相关。

48
	第五章系统实现
	




业务逻辑:读取文档信息,对文档信息中各个字段做倒排索引,根据不同

的字段查询需求判断是否同时将字段信息存入倒排索引文件库中。完成倒排索

引创建后将其持久化更新到索引文件库中。图5-6为文档索引业务逻辑图。

IK分词
	构述倒排索引 Cllizj〉 保存索iJI

图5-6文挡索引业务逻辑图

相关代码:

/**

*创建倒排索引

*/

private void addDocToIndex(JSONObject docInfo){

document.add(new Field("title", doc.getString("titIe"), Field.Store.NO,

Field.Index.NOT—ANALYZED—NO—NORMS));

document.adcl(new Field("content", doc.getString("content"), Field.Store.NO,

Field. Index.NOT_ANALYZED_NO_NORMS));

//以下添加或更新索引

try {

Index Writer index Writer = getlndexWriterQ ;

if(row.equa]s("") || rowKey.equals(""))

index Writer.addDocument(document);

else

index Writer. updateDocument(new Term(rowKey, row), document);

hitCounts[index] ++;

} catch (CorruptlndexException e) {

logger[index].log(Level. WARNING, ”添加或更新 documnet 到 index Writer

出错:”+e.getMessage());

} catch (lOException e) {

49
	基于网络爬虫的信息采集分类系统设计与实现
	




logger[index].log(Level.WARNING,``添加或更新 documnet 到 indexWriter

出错:"+e.getM[essage());

}

}

该方法实现对文档各字段的倒排索引创建,对文档建立倒排索引,方面前

台页面搜索时快速定位到文档位置。

*从oracle tr—word_dict中加载扩展词典

*	@throws SQLException

*	@throws Exception

*/

在实际业务中,由于业务的独特性,自带分词字典可能无法满足准确的分

词要求,该方法提供扩展分词的加载,满足特定需求下的分词准确性。

5.2.3 WEB 展示

1、采集任务队列配置增删改查

任务队列是控制爬虫数据釆集的首要因素,需提供各大主要参数的配置界

面,页面设计如下图所示:字段包含名称、任务队列id、优先级(数值越大,

优先级越高)、其实任务位置(0代表从头开始)、时间规则(cron表达式)、任

务来源;操作包含启停按钮,编辑按钮、添加按钮和删除按钮。

.25S
	、,.S&kTS"

JOB管理

名称:
	_册务 ~|

任号?人列!D:	bfTask

....` 伏先织:
	5

起始(壬务:
	258

时间綱:
	0?…?

任务来源:	re<Jis 0

ite^ #55?

..:?

amsi.s'
	□

图5-7任务队列配置页面

50
	第五章系统实现
	




2、釆集任务配置增删改查

任务是爬虫数据采集的重要依据,为实现自由扩展数据范围,提供任务增

删改查页面配置,页面设计如下图所示:字段包含id、名称、网站、所属任务

队列、所属网站频道、数据抽取模板、链接地址url;操作包添加按钮,编辑按

钮、删除按钮、批量移动、批量删除等待,其中编辑面板中增加运行按钮方面

立即执行该任务。

薩藤 □□

`:' ;B : "V `: □ `:-M

11:协会
	bglask
	f办会幼态
	13荑

Ik协会
	bgT ask
	新闻中心
	CiS ^

ikte任务管理
	k M

嚷

业协5 ID:
	|7d30a7327bde482d8ab3438d3de| BBtBTTBI IHmiilM

;
	,~~~,

m 任务队列:	请迭择0

每团 名称:
	宝iR任夯

网站:
	中国稀土行ik协会

1|团

频道:
	协会动态

网站祺扳:
	宝洞中国稀土行li:协会文档列表1

f维

^ pftgeType:	bg_a.c-rei_1 Jist

f_ URL:
	http://www.ac-rei.org.cn/portaUphp?mod?U5t&catid=4

m

哼KEY
	VALUE
	^

fig

图5-8任务配置页面

3、数据抽取规则模板增删改查

规则模板用于数据的抽取,实现数据精准定位,保存实实在在需要的数据,

提供规则模板配置页面,页面设计如下图所示:字段名称、内容;操作包添加

按钮,编辑按钮、删除按钮、搜索、导入导出、验证等等,内容以xml树状文

本格式进行编辑,后台存储采用keyvalue形式保存,其他按钮很容易明白意思,

这里的验证按钮是提供模板编辑完之后,数据抽取是否准确的一个验证通道,

通过验证,保证模板正确服务于采集系统。

51
	基于网络爬虫的信息釆集分类系统设计与实现
	




“.?: . □ ;`
 ::
 ....... :'


KB'
	. .; feBiS 1MM MM

宝!R中国稀土行业协会文档列衷1
	U.J X

察 摸IS管理
	se 5-

名称:
	宝5R中国稀土行it办

	 . 
			. ....
			 : 0]
<?xmi version=" 1. 0" encodirig= 'vUTF-8'v ?>
	*

<page pag&I>"pe='ybg_ac—rei_l__lis"t">

<uielement s ob.3ectI'J-a3Tte= A'bef0reSemailt i?
	E

〈u 1 ob j e ct f i ? 1 dS ame=11 dyD om":

〈express ±on><\ [CDATA[:| |j | ;
</uiobject>

</uie:lement s>

<13 i e 1 emen t s b :i e ci R"ajne="" do ctjmeiit T ask ".......

:rows>< \ CCDATA [$R013 (t idyDom, ' S |

<uiobject f 1.?idl'JajTi.e= “`ur 1nul.

11
	^Isxpress'ionX! [CPATA[^ j

</uiobject>

:: 模fS内S :
	<uiobject f ie 1 dM.ame="`v.?ebsiten:丨

<expressiort>< I [CDATA [: I

</\iiob j ect >

<u.iobject f:Le-ldN>:iaie=“pageType'	,
? rrn5 t0 r*""

A

rtttr 1T* tssSfTH -I'l
	?
	1 i ?T"%
	lijr-i

图5-9抽取模板配置页面

4、结果展示全文检索

全文检索,即根据关键词查找分析结果的所有文档,搜索按钮提交搜索框

中的关键词,查询分析过程中建立的索引文件(分析过程中对所有文档都建立

了倒排索引),找出关键词对应的所有文档,图5-10为全文检索页面展示图。

铝
	搜索

0相关文章11435篌

,職分离机的环保性能

^海关纖布1明份腿出口数提

?地方?府干预绍价闻风下行

^电麵存猶续 加

`铝稱怕勺分类及用途

;2012中国_□置或谓6.1%

`風翻合金窗价格-凤□台金窗纖-黯金窗浦

图5-10全文检索页面

52
	第五章系统实现
	




5、分类菜单展示

根据业务分类需求,将所有分类产生一个分类菜单,并按要求实现父子节

点的关联,点击菜单各个节点,将展示分析过程中置于该分类下的所有文档,

其中点击父节点,则展示该父节点下所有符合子节点的文档。根据归类展示的

数据,将同类数据集中一起,方便与其他二次分析。

_ 
_
—
—
—
—
—
—
—~
~□
~-
—
—~
~
—




,t ,	O 5:1O0:

横is东驗縣司._公司详領,里巴巴

横店东il:公5S是铁氧瘩?材的賴失制渣商-莰点M经

`p ? ; ,
	楮土永磁再度火锡3股涨?傳或回光返照ig头条三?每曰经济新闻
Vr-S**i	^稀細赃職多欠-權网

mm
	細钱棚行ii再分析-深训木头的曰志-网易博客

H	細铁棚行让再分祈j训木头—百度空间


新材料板块:政策IS上腾飞翅腠-24小时滚动鋪闻-人民网

-体_永磁体谢各—优质永磁体批发《购-阿里巴巴

关子提畐钦磁铁氧体出口遇稅建设和呼吁的公示

`中国电子元件行业协会磁性材料与器件分会苐七届理事会组成名单

中国电子元件行业协会敏丨生材料与器件分会第七届理辜会组成名单

□中国耐桐-磁材欧_永磁歌si材料丨纖鏹藏性材料眩铁?

图5-11菜单分类页面

5.3系统测试

本章节主要对系统各模板功能做分析测试,测试系统功能正确性,完整性

以及性能情况。检查整个系统对预期目标的完成情况。

功能性测试

53
	基于网络爬虫的信息采集分类系统设计与实现
	




表5-1爬虫模块测试

测试项g 丨测试过程描述	|预计g标|实际效果 丨补充说明

配置界面管理通过界面对各管理模块完全正确完全正确

功能
	做数据操作,同时检查

Redis相应数据是否正

确修改。

任务优先级 配置连个不同优先级的完全正确完全正确

人物队列,检查爬虫组

件获取的任务是否符合

优先级效果。
	




采集功能 分别配置不同网站,不完全正确 99%正确 极个别网

同编码格式等各种情况
	站服务器

下的网站链接任务,检
	不稳定导

查采集到的网页源码是
	致无法获

否正确。
	取数据

数据抽取功能配置相应网站抽取数据完全正确完全正确

的规则模板,检查数据

抽取结果的正确性和完

整性。

保存结果验证保存数据到HBase,同完全正确完全正确

时查找HBase中实际存

入数据是否有误

54
	第五章系统实现
	




		表5-2分析和数据展示模块测试	
	




测试项巨 测试过程描述|预计B标 实际效果 补充说明

分词准确性 分词器部分分100%正确分95%正确分词这里涉及专有

别做词语,短词
	名词等特殊词

句字,长句子,
	群分词有误情

文字各情况下
	况

的分词,比对

分词结果是否

达到预期目标
	




分类算法 检查分类结果100%符合要80%符合分类有待算法的不

中的文字是否求
	要求
	断优化

符合预定分类

要求

索引功能 创建一定文章完全正确 完全正确

集合的索引,

再对其进行索

引查询,验证

查询结果

数据展示界面在实际数据正完全正确 完全正确

功能
	确的情况下检

查各分类菜单

数据展示是否

正确

	^
	I
	I
	^
	




55
	基于网络爬虫的信息采集分类系统设计与实现
	




性能测试

表S-3系统性能测试

—测试项目 |测试过程描述|预计目标 |实际效果 |补充说明

页面采集效率测试单个任务单个页面响应大部分网站符个别冷面网页

不同网站下的在100毫秒左合性能需求 服务器响应速

采集耗时;多右,不等超过
	度慢导致采集

个任务同一网1000毫秒
	效率下降

站下的采集耗

时

数据抽取效率测试大页面多2000个字段/s满足要求

字段抽取、大

页面少字段抽

取、小页面多

字段抽取、小

页面少字段抽

取四种情况下

分别耗时

文本分析效率爬虫开启到最比爬虫釆集 满足要求

大釆集速度,快,对爬虫采

查看是否有信集到的数据能

息堆积未处理及时快速处理

	的情况。	掉	
	




5.4系统出错处理

5.4.1出错信息

1、网络异常,浏览器连接出错。

2、数据库连接出错。

3、用户提供的信息出错时,应作出正确的处理。

56
	第五章系统实现
	




5.4.2补救措施

1、系统内部可能出现各种异常、出错,为考虑出错统一处理,所有出错

信息给予用户一个页面提示,统一弹出errorjsp页面。

2、若是业务异常信息,如用户输入出错,操作出错,将给予弹窗提示。

5.5本章小结

本章节主要针对系统各个模块进行详细的 发实现,包括采集系统的配置

界面开发,任务调度逻辑开发,采集逻辑,数据抽取,数据保存等的实现;数

据分析方面重点介绍分类逻辑算法实现以及倒排全文索引文件的建立;web展

示部分介绍索引查询界面的实现。最后介绍系统异常出错信息以及相关的补救

措施。

57
	基于网络爬虫的信息采集分类系统设计与实现
	




第六章总结与展望

6.1总结

该系统是一个互联网数据分析系统,通过互联网采集工具爬虫,结合自然

语言分析算法,lucene搜索技术,并最终展示分析挖掘后的数据,在该项目中

体现了互联网数据分析关于采集,分析,展示的几个环节。在这期间,根据课

题的定义及预定达到的目标,进行了系统整体的可行性分析、功能性能分析,

并进行各个功能模块的设计,数据字典设计,根据设计文档进行编码实现,最

后进行联调测试完成整个系统研发。该系统能够更加配置,获取互联网范围内

的数据,并通过数据分析模块对文本进行分类和构建索引,最后通过Web界面

进行展示分类数据效果。基本能达到预定的分类效果,即分类下的数据90%以

上符合分类要求;不足之处在于互联网数据的随意性以及文本分析的局限性,

导致出现部分数据不符要求。

6.2展望

互联网数据分析是未来发展的趋势,本系统是对该领域的一个实践。同时,

该系统在设计上,充分考虑其易扩展性需求,在爬虫模块,数据分析模块上做

了很好业务兼容性:定向爬取的思想可以很容易配置不同业务需求的数据源,

这样该系统可以应用在不同行业领域的数据采集,同时分析模块是组件化独立

模块,运行于统一的分布式流计算框架中,其他业务只需根据需求 发分析算

法即可。改系统数据挖掘功能还相对单薄,可在此基础上扩展深入研发数据挖

掘算法,使其更加符合生产业务需求。

58
	参考文献
	




参考文献

[1]王学松.11?咖6+1她:11搜索引擎开发[M].北京:人民邮电出版社,2008:3.

[2]陈宁.Lucene全文检索在网络教学平台中的应用研究[d].大连:大连海事大学,2007

[3]	the apache Jakarta project. Lucene [eb/ol].

http..jakarta.apache.org//lucene,2010-05-21 ?

[4]车东.Licene:基于java的全文检索引擎简介[eb/ol].

http://www.chedong.com/tecMucene.htmL 2010-5-18.

[5]徐明华.Java	Web 整合开发与项目实战:JSP, Ajax,Structs, Hibernate. Spring[M].北

京:人民邮电出版社,2009.2:234-333

[6]程伟.汪孝宜.开发者突击:Jsp网络开发经典模块实现全集[M].北京:电子工业出版

社,2008.963

[7]飞思科技产品研发中心编著.JSP应用开发详解[M].电子工业出版社,2002-1.

[8]尹江,尹治本,黄洪.网络爬虫效率瓶颈的分析与解决方案[J].北京理工大学学报,2004,

(10):33-41.

[9]李勇,韩亮.主题搜索引擎中网络爬虫的搜索策略研究[J].计算机工程与科学,2008,
	“


(03):44-46.

[10]汪涛,樊孝忠.主题爬虫的设计与实现[J].计算机应用,2004,	(Sl):110-121

[11]李刚.轻量级Java	EE企业应用实战-Struts 2+Spring+Hibernate整合开发北京:电

子工业出版社,2009.9:68-72

[12]孙卫琴:精通Hibernate:	Java对象持久化技术详解[M].电子工业出版社.

[13]罗刚.解密搜索引擎技术实战:1^6!^&如3精华版.电子工业出版社.2011-6-1

[14]马晓玉.Oracle	10g数据库管理应用与开发.清华大学出版社.2007-11-1

[15]杨忠明.Oracle	10g SQL和PL/SQL编程指南?清华大学出版社.2009-1-1

[16]吴众欣,沈家立.Lucene分析与应用.机械工业出版社.2008-9-1

59
	
	




致谢

本课题在选题及研究过程中得到我的导师刘昆宏副教授的悉心指导,老师

多次询问课题的研究进程,几个月来不厌其烦地为我解答研究中以及论文写作

过程中的难题,同时帮我详细的指导课题研究的理论问题、实际应用等,从选

题到完成,每一步都倾注了刘老师大量的心血。他帮助我开拓了研究思路,丰

富了研究方法。刘老师一丝不苟的作风,严谨求实的态度,诲人不倦的高尚师

德,朴实无华、平易近人的人格魅力对我影响深远。在此对刘老师的细心指导

和关怀表示衷心的感谢。

本论文的完成也离不 软件学院各位老师关心与帮助,特别是学习软件专

业研究生课程一年多来,耳闻目染了学院各位老师的敬业精神,他们潜心钻研、

无私奉献的态度将赋予我终生受益的无穷之道。与此同时,在这不长的学习经

历中,有幸结识了来自社会各界不同岗位上工作的同学,他们身上刻苦的求知

精神是我不断学习的榜样。在此向所有的老师和同学们表示谢意与祝福,谢谢

你们!

60
